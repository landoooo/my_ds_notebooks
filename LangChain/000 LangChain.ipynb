{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead40eac-6e73-4e07-a776-369c2f76ef09",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "**LangChain** is an opensource framework for connecting LLMs and datasources under a unified syntax. \n",
    "\n",
    "It allows the creation of scalable modular aplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83357f5-4c4e-4ff0-b83f-63687608b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85169a48-76ad-403f-be5a-754faa4c54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=os.getenv('HUGGINGFACE_API_KEY'))\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = 'Whatever you do, take care of your shoes'\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b9cf2-20f9-4c44-a65b-5a6696cb4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "lm = OpenAI(model=\"gpt-3.5-turbo-instruct\", api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = 'Whatever you do, take care of your shoes'\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12213f1d-056f-48f3-abbe-71b77e4aadbe",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbaf295-95b7-43f5-9c6b-6fde33714b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = 'You are an artificial intelligence assistant, answer the question. {question}'\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['question'])\n",
    "\n",
    "print(prompt_template.invoke({'question': 'What is LangChain?'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda40a78-4a00-45f5-aa66-45d805d12cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'You are soto zen master Roshi.'), \n",
    "        ('human', 'What is the essence of Zen?'),\n",
    "        ('ai', 'When you are hungry, eat. When you are tired, sleep.'),\n",
    "        ('human', 'Respond to the question: {question}'),   \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3070d1-f092-4638-a110-b96cfd7ad6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-4o-mini', api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "llm_chain = prompt_template | llm \n",
    "question = 'what is the sound of one hand clapping?' \n",
    "response = llm_chain.invoke({'question': question})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96def8f4-1fe7-42b4-b799-bdcf68b038c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template from the template string\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "\n",
    "# Create a chain to integrate the prompt template and LLM\n",
    "llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=os.getenv('HUGGINGFACE_API_KEY'))\n",
    "llm_chain = prompt | llm \n",
    "\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "print(llm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e21a2d-e150-4ed1-b1f7-87748cd4be80",
   "metadata": {},
   "source": [
    "# Managing chat model memory \n",
    "\n",
    "Allows: \n",
    "- follow up questions\n",
    "- response iteration and expansion\n",
    "- personalization\n",
    "\n",
    "Limited by the model context window (amount of input text a model can consider at once when generating a response)\n",
    "\n",
    "Langchain has 3 classes to model this context window: \n",
    "- ChatMessageHistory: full exchange of messages.\n",
    "- ConversationBufferMemory: keeps a certain number amount of messages \n",
    "- ConversationSummaryMemory: keeps a summary of the conversation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91236e-a86a-449d-b3fc-ee1878e35b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory \n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "history = ChatMessageHistory() \n",
    "history.add_ai_message('Hi! Ask me anything about langchain.')\n",
    "history.add_user_message('Describe a metaphor for learning LangChain in one sentence.')\n",
    "\n",
    "response = llm.invoke(history.messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b96d6-2903-46f9-bb69-29f1ad4f816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_user_message('Sumarize the preceding sentence in fewer words')\n",
    "response = llm.invoke(history.messages) \n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb92493-550c-40cd-a170-a740a80fe549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain.chains import ConversationChain \n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-40-mini', api_key = os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "memory = ConversationBufferMemory(size=4) \n",
    "buffer_chain = ConversationChain(llm=llm, memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c92b0d-d4d8-454d-9c5a-dbf4b8dd7277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d672054-d9a1-4822-9c35-48623a5f64d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Define a summary memory that uses an OpenAI chat model\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "summary_chain = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "summary_chain.invoke(\"Describe the relationship of the human mind with the keyboard when taking a great online class.\")\n",
    "summary_chain.invoke(\"Use an analogy to describe it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570016db-2a03-4d81-8ac3-adbe6a20c3f2",
   "metadata": {},
   "source": [
    "# Sequential Chains\n",
    "\n",
    "In sequential chains the output of one chain becomes the input of the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f61031-52c7-485b-b99b-aa5a41818158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template that takes an input activity\n",
    "learning_prompt = PromptTemplate(\n",
    "    input_variables=[\"activity\"],\n",
    "    template=\"I want to learn how to {activity}. Can you suggest how I can learn this step-by-step?\"\n",
    ")\n",
    "\n",
    "# Create a prompt template that places a time constraint on the output\n",
    "time_prompt = PromptTemplate(\n",
    "    input_variables=['learning_plan'],\n",
    "    template=\"I only have one week. Can you create a plan to help me hit this goal: {learning_plan}.\"\n",
    ")\n",
    "\n",
    "# Invoke the learning_prompt with an activity\n",
    "print(learning_prompt.invoke({\"activity\": \"Swimming\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01b4b2-cf14-4b6f-a6b3-9e8caadc82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "learning_prompt = PromptTemplate(\n",
    "    input_variables=[\"activity\"],\n",
    "    template=\"I want to learn how to {activity}. Can you suggest how I can learn this step-by-step?\"\n",
    ")\n",
    "\n",
    "time_prompt = PromptTemplate(\n",
    "    input_variables=[\"learning_plan\"],\n",
    "    template=\"I only have one week. Can you create a plan to help me hit this goal: {learning_plan}.\"\n",
    ")\n",
    "\n",
    "# Complete the sequential chain with LCEL\n",
    "seq_chain = ({\"learning_plan\": learning_prompt | llm | StrOutputParser()}\n",
    "    | time_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "# Call the chain\n",
    "print(seq_chain.invoke({\"activity\": \"fishing\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed627418-259b-4ece-b3f0-c4d8a23ce7a2",
   "metadata": {},
   "source": [
    "# Agents \n",
    "\n",
    "Agents use LLMs to take actions \n",
    "\n",
    "Tools are functions called by the agent \n",
    "\n",
    "ReAct (Reasoning and Acting) agents are a type of agents\n",
    "\n",
    "LangGraph is a branch of langChain centered around agents systems, decoupling our implementation from any specific vendor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a2a30-3081-402e-80fa-23a891c23fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent \n",
    "from langchain.agents import load_tools \n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', api_key=os.getenv('OPENAI_API_KEY'))\n",
    "tools = load_tools(['llm-math'], llm=llm)\n",
    "agent =create_react_agent(llm, tools)\n",
    "\n",
    "messages = agent.invoke({'messages': [('human', 'What is the square root of 101?')]})\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8013f-fdfc-492a-a550-9c6eb8a99af5",
   "metadata": {},
   "source": [
    "# Creating tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a2771-d6c0-4ab6-b2d5-43136c5c5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "tools = load_tools(['llm-math'], llm=llm) \n",
    "print(tools[0].name)\n",
    "print(tools[0].description) #The llm will invoke this tool depending on its description \n",
    "print(tools[0].return_direct) # Indicates if the LLM should stop after invoking this tool \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df3d3e-ffc9-4ca6-909f-c0663d5c6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def financial_report(company_name:str, revenue:int, expenses:int) -> str:\n",
    "    '''Generate a financial report for a company that calculates net income.''' \n",
    "    \n",
    "    net_income = revenue - expenses \n",
    "    \n",
    "    report = f'Financial report for {company_name}:\\n'\n",
    "    report += f'Revenue: ${revenue}\\n'\n",
    "    report += f'expenses: ${expenses}\\n'\n",
    "    report += f'Net income: ${net_income}\\n'\n",
    "    \n",
    "    return report\n",
    "\n",
    "print(financial_report('Totos motos', 3245345, 345345))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2599ba5-6752-4541-9093-82ee2d864d30",
   "metadata": {},
   "source": [
    "Lets create a tool out of that function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f89fa-a670-4b8b-882c-9af96e6463c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool \n",
    "\n",
    "@tool \n",
    "def financial_report(company_name:str, revenue:int, expenses:int) -> str:\n",
    "    '''Generate a financial report for a company that calculates net income.''' \n",
    "    \n",
    "    net_income = revenue - expenses \n",
    "    \n",
    "    report = f'Financial report for {company_name}:\\n'\n",
    "    report += f'Revenue: ${revenue}\\n'\n",
    "    report += f'expenses: ${expenses}\\n'\n",
    "    report += f'Net income: ${net_income}\\n'\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "print(financial_report.name)\n",
    "print(financial_report.description) #The llm will invoke this tool depending on its description \n",
    "print(financial_report.return_direct) # Indicates if the LLM should stop after invoking this financial_report\n",
    "print(financial_report.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7af8d-f537-442d-bf2f-9e96c41551df",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) \n",
    "\n",
    "Pretrined LLM dont have access to external data outside of the training one.\n",
    "\n",
    "RAG is a technique to allow the LLM to use external data. It makes use of embeddings to retrieve relevant information to integrate into the prompt. \n",
    "\n",
    "In langchain, there are 3 steps to use RAG: \n",
    "- Document Loader: langchain provides document loaders for common file types like pdf, csv... 3rd parties allow for other file types loaders.\n",
    "- Splitting (information into chunks): to avoid missing context, there is often a bit of chunk overlap. The splitting strategy as well as the chunck size will depend on the problem at hand.\n",
    "- Storage + Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24049da5-97ea-44f5-8860-cfac3b647620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "loader = PyPDFLoader(\"/Users/el_fer/Downloads/Confidences d’un petit cochon (Pierrette Dubé) (Z-Library).pdf\")\n",
    "\n",
    "data=loader.load() \n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa628dc-2a8d-4847-8720-8cc61dce09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter \n",
    "\n",
    "quote = 'one machine can do the work of fifty ordinary humans. no machine can do the work of one extraordinary human'\n",
    "ct_splitter = CharacterTextSplitter(\n",
    "    separator = '.', \n",
    "    chunk_size = 20, \n",
    "    chunk_overlap=4\n",
    ")\n",
    "\n",
    "docs =ct_splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f06af37-0d9b-44f8-8eff-da5914dfcc94",
   "metadata": {},
   "source": [
    "The problem of this method is that it uses present separators to split the text. \n",
    "\n",
    "Lets try to find a more robust splitting technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae9be1-9134-4441-93b0-518b153f2991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=24, \n",
    "    chunk_overlap=3\n",
    ")\n",
    "\n",
    "docs = rc_splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03266189-a904-4ebe-b659-9b748fc27a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma \n",
    "\n",
    "# PENDING: create the documents to be stored\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(api_key=os.getenv('OPENAI_API_KEY'), model='text-embedding-3-small')\n",
    "vectorstore = Chroma.from_documents(docs, \n",
    "                                   embedding=embedding_function, \n",
    "                                   persist_directory='./chromaDB')\n",
    "\n",
    "retriever = vectorstore.a_retriever(\n",
    "    search_type='similarity', \n",
    "    search_kwargs={'k': 2}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b79224-3d08-440d-a7ac-35f006e65d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = '''\n",
    "Review and fix the following TechStack marketing copy with the following guidelines in consideration: \n",
    "Guidelines: \n",
    "{guidelines} \n",
    "\n",
    "Copy: \n",
    "{copy} \n",
    "\n",
    "Fixed Copy:\n",
    "'''\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(['human', message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d863006-62c6-4557-92a3-774013f4df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough \n",
    "\n",
    "rag_chain = ({'guidelines': retriever, 'copy': RunnablePassthrough()}\n",
    "             | prompt_template \n",
    "             | llm) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
