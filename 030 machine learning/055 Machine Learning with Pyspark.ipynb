{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9bcce13-a232-493a-b480-57dd94ffb0a0",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca996653-e5ed-4807-b77e-0a060702c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d8e3d-0fcb-4a0b-9158-5bd682fb480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf730ed1-1c2f-4533-b0f9-87cc393e5a7f",
   "metadata": {},
   "source": [
    "## Creating a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def1c94-ce05-493a-bd56-80f05403f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local[1]').appName('testing_spark').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e98095-d3e8-49c9-ab50-d1705d787536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good practice is to stop the cluster \n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5beb8b-7932-4d53-be60-5eded2c92af9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T14:39:45.498492Z",
     "iopub.status.busy": "2024-09-27T14:39:45.498051Z",
     "iopub.status.idle": "2024-09-27T14:39:45.605553Z",
     "shell.execute_reply": "2024-09-27T14:39:45.604949Z",
     "shell.execute_reply.started": "2024-09-27T14:39:45.498462Z"
    }
   },
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf0a71-0efb-451a-a8b9-452a820184cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "data = [\n",
    "    (1, \"John\", 28, 175.5),\n",
    "    (2, \"Anna\", 23, 160.2),\n",
    "    (3, \"Mike\", 35, 180.3)\n",
    "]\n",
    "\n",
    "# Define the schema with specific data types\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Height\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create the DataFrame with the defined schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Print the schema to verify data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540801db-e5aa-4fa5-b37f-d40a97c78932",
   "metadata": {},
   "source": [
    "## Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c500aa-6aaa-44b9-ac90-9521b3b61a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = spark.read.csv('data/cars.csv', header=True)\n",
    "cars.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca54a1-56e2-45f9-8ca9-3a02daa8fbb5",
   "metadata": {},
   "source": [
    "Loading data has the following arguments: \n",
    "- header: is the first ow a header?\n",
    "- sep: field separator\n",
    "- schema: explicit column data types\n",
    "- inferSchema: deduca column data types from data\n",
    "- nullValue: placeholder for missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220c841-c1d6-4200-9017-49f062a945ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4daee2d-03e4-4413-b7e9-54345ee7ad52",
   "metadata": {},
   "source": [
    "At this point everything is a string in this dataframe!! \n",
    "\n",
    "We can let spark infer the data types of each column or explicitly define their types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bb7da-02bb-488c-9f9d-c48af8e19e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = spark.read.csv('data/cars.csv', header=True, inferSchema=True)\n",
    "cars.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d674c-c99e-4886-9943-529eb2bbce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7dc577-e71f-4f0c-a17c-54ea411e007c",
   "metadata": {},
   "source": [
    "Null values could mislead spark and assign string due to the presence of 'NA' or similar strings.\n",
    "\n",
    "If automatic type inference is not applicable we can explicitly declare the data types like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784646c4-aad4-4585-a63a-a8ee9b2594e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "data = [\n",
    "    (1, \"John\", 28, 175.5),\n",
    "    (2, \"Anna\", 23, 160.2),\n",
    "    (3, \"Mike\", 35, 180.3)\n",
    "]\n",
    "\n",
    "# Define the schema with specific data types\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Height\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create the DataFrame with the defined schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Print the schema to verify data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd17d1-109c-4f29-96bb-8cd6a3e5eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv('data/sms.csv', sep=';', header=False, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ff85e-3e67-4b4f-85c7-3223a7d430dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "flights = spark.read.csv('data/flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a4b85-d781-4663-9606-24b4ab454e2c",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0319425c-0b73-431e-92dd-98c523564eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = spark.read.csv('data/auto.csv', header=True, inferSchema=True)\n",
    "cars.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abc3f4-4f0f-4bda-a548-773607289dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.limit(10).toPandas().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f43fce-5928-4080-8b31-02b12f51f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars.drop('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7f367-ad66-413d-bf4c-975fa399d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars.select('origin', 'cylinders', 'weight', 'horsepower', 'mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201bc7f8-01a3-4c2e-90d4-556eee178106",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.filter('cylinders IS NULL').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde9457-c5b9-49fe-b3a8-b00924bf1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.filter('weight IS NULL').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eee159-79f0-4809-8c09-5d09f200e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.filter('mpg IS NULL').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e5eb6-b878-47f6-a744-e31caac65042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "cars = cars.withColumn('mass', round(cars.weight/2.205, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109c4cc-a99f-4502-ab53-f42e8e6e882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6c602-8b46-4282-9fc3-5fc09dcb9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer \n",
    "\n",
    "indexer = StringIndexer(inputCol='origin', outputCol='origin_idx')\n",
    "\n",
    "indexer = indexer.fit(cars)\n",
    "\n",
    "cars = indexer.transform(cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a4faa-461c-4171-ba28-c61a8583f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.limit(100).toPandas().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb062cc-2855-4571-8a03-04bc994a932e",
   "metadata": {},
   "source": [
    "The final step in preparing the cars data is to consolidate the various input columns into a single column. This is necessary because the Machine Learning algorithms in Spark operate on a single vector of predictors, although each element in that vector may consist of multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad3502-336d-496e-9b43-91707e05c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "assembler = VectorAssembler(inputCols=['cylinders', 'origin_idx'], outputCol = 'features')\n",
    "cars = assembler.transform(cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f965f-cd5d-4f2d-a70d-7a18034ba85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.limit(100).toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd6e28-1505-4e49-8a88-f03ae7f495ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "flights_drop_column.filter('delay IS NULL').count()\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bbf632-03a4-428c-a50a-248eb1f6535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column (1 mile is equivalent to 1.60934 km)\n",
    "flights_km = flights.withColumn('km', round(flights.mile * 1.60934, 0)) \\\n",
    "                    .drop('mile')\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "flights_km.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586e259-b486-4230-943a-283b33fa49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(flights_km)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights_km)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
    "flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6204f16-fa9d-4983-b3df-287f6ac7211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow','carrier_idx','org_idx','km','depart','duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_indexed)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db75c9f-12c0-4246-b9da-c55f0c5e3279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
