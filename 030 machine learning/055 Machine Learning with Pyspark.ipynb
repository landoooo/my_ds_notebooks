{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9bcce13-a232-493a-b480-57dd94ffb0a0",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca996653-e5ed-4807-b77e-0a060702c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d8e3d-0fcb-4a0b-9158-5bd682fb480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf730ed1-1c2f-4533-b0f9-87cc393e5a7f",
   "metadata": {},
   "source": [
    "## Creating a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def1c94-ce05-493a-bd56-80f05403f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local[1]').appName('testing_spark').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e98095-d3e8-49c9-ab50-d1705d787536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good practice is to stop the cluster \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5beb8b-7932-4d53-be60-5eded2c92af9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T14:39:45.498492Z",
     "iopub.status.busy": "2024-09-27T14:39:45.498051Z",
     "iopub.status.idle": "2024-09-27T14:39:45.605553Z",
     "shell.execute_reply": "2024-09-27T14:39:45.604949Z",
     "shell.execute_reply.started": "2024-09-27T14:39:45.498462Z"
    }
   },
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf0a71-0efb-451a-a8b9-452a820184cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "data = [\n",
    "    (1, \"John\", 28, 175.5),\n",
    "    (2, \"Anna\", 23, 160.2),\n",
    "    (3, \"Mike\", 35, 180.3)\n",
    "]\n",
    "\n",
    "# Define the schema with specific data types\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Height\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create the DataFrame with the defined schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Print the schema to verify data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540801db-e5aa-4fa5-b37f-d40a97c78932",
   "metadata": {},
   "source": [
    "## Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c500aa-6aaa-44b9-ac90-9521b3b61a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = spark.read.csv('data/cars.csv', header=True)\n",
    "cars.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca54a1-56e2-45f9-8ca9-3a02daa8fbb5",
   "metadata": {},
   "source": [
    "Loading data has the following arguments: \n",
    "- header: is the first ow a header?\n",
    "- sep: field separator\n",
    "- schema: explicit column data types\n",
    "- inferSchema: deduca column data types from data\n",
    "- nullValue: placeholder for missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220c841-c1d6-4200-9017-49f062a945ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4daee2d-03e4-4413-b7e9-54345ee7ad52",
   "metadata": {},
   "source": [
    "At this point everything is a string in this dataframe!! \n",
    "\n",
    "We can let spark infer the data types of each column or explicitly define their types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bb7da-02bb-488c-9f9d-c48af8e19e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = spark.read.csv('data/cars.csv', header=True, inferSchema=True)\n",
    "cars.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d674c-c99e-4886-9943-529eb2bbce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7dc577-e71f-4f0c-a17c-54ea411e007c",
   "metadata": {},
   "source": [
    "Null values could mislead spark and assign string due to the presence of 'NA' or similar strings.\n",
    "\n",
    "If automatic type inference is not applicable we can explicitly declare the data types like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784646c4-aad4-4585-a63a-a8ee9b2594e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "data = [\n",
    "    (1, \"John\", 28, 175.5),\n",
    "    (2, \"Anna\", 23, 160.2),\n",
    "    (3, \"Mike\", 35, 180.3)\n",
    "]\n",
    "\n",
    "# Define the schema with specific data types\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Height\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create the DataFrame with the defined schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Print the schema to verify data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd17d1-109c-4f29-96bb-8cd6a3e5eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv('data/sms.csv', sep=';', header=False, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ff85e-3e67-4b4f-85c7-3223a7d430dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "flights = spark.read.csv('data/flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d67ce6-f928-43fd-9b31-64d007959721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
