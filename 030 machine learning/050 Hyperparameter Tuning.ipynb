{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf100a2c-5c03-40f4-913a-987b6e1db802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import warnings \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b28017-6ed7-4f2c-b71a-9d49b8a08699",
   "metadata": {},
   "source": [
    "# Parameters VS Hyperparameters\n",
    "\n",
    "## Parameters\n",
    "\n",
    "**Parameters** are components of the model learned during the modeling process. \n",
    "\n",
    "You cannot set them. They are discovered by the algorithm.\n",
    "\n",
    "In linear models these parameters are the coefficients and the intercept. In tree based algorithms these params are node decisionas (feature and value to split on)\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "**Hyperparameters** are something you set before the modeling process \n",
    "\n",
    "The algorithm does not learn these during the training\n",
    "\n",
    "Some hyperparameters are more important than others.\n",
    "\n",
    "To find the parameters of a model, create one and print it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947eb134-a282-4b78-8274-ebef13f754f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier() \n",
    "rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea51db-9e98-45eb-90e2-272e02a22898",
   "metadata": {},
   "source": [
    "To know the meaning of each parameter, go to the sklearn documentation page \n",
    "\n",
    "Some hyperparameters are more important than others. Some wont help model performance (the ones about keeping information about the training, the verbosity level or the random state for instance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7f6b8-9d09-425b-a701-01e81733fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/credit-card-full.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05029a85-dfae-44db-9b85-a67b3ffeb59f",
   "metadata": {},
   "source": [
    "# Hyperparameter Values\n",
    "\n",
    "There is no one size that fits all. The values for each hyperparameter depend on the algorithm and hyperparameter. There are some best practices for them though.\n",
    "\n",
    "## Conflicting Values\n",
    "\n",
    "Be aware of conflicting hyperparameter choices. This should be clearly stated in the sklearn documentation.\n",
    "\n",
    "## Silly Values\n",
    "\n",
    "There are values for different algorithms that most likely wont yield any good result like low number of trees in a random forest model, one neighbor in KNN, or increment a hyperparameter by a very small value...\n",
    "\n",
    "# Automating Hyperparameter Choice\n",
    "\n",
    "It an be automated with for loops over arrays of possible values, storing scores, but it becomes unmanageble when we consider multiple hyperparameters\n",
    "\n",
    "## Grid Search \n",
    "\n",
    "We could nest a loop per hyperparameter to consider, what would increase the computational charge in an exponentional manner.\n",
    "\n",
    "Grid search consist in train a model for each combination of values for the different hyperparameters.\n",
    "\n",
    "### Pro's \n",
    "\n",
    "You dont have to write thousands of lines of code\n",
    "\n",
    "The best model within the grid values is guaranteed to be found (unless silly or conflicting values are there)\n",
    "\n",
    "### Cons \n",
    "\n",
    "Computationally expensive\n",
    "\n",
    "Its uninformed: each model doesnt take into account the outcome of previous trainings. \n",
    "\n",
    "## Grid Search in SciKit Learn\n",
    "\n",
    "Steps to perform grid search: \n",
    "1. Choose an algortithm to tune its hyperparameters (estimator)\n",
    "2. Define the hyperparameters to be tuned\n",
    "3. Define range of values for each hyperparameter\n",
    "4. setting a cross validation scheme\n",
    "5. define a scoring function\n",
    "6. include extra useful information or functions\n",
    "\n",
    "[sklearn GridSearchCV documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c393649f-8ed3-454a-9cbe-e86235bf3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest Classifier with specified criterion\n",
    "rf_class = RandomForestClassifier(criterion='entropy')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']} \n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_rf_class = GridSearchCV(\n",
    "    estimator=rf_class,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    cv=5,\n",
    "    refit=True, return_train_score=True)\n",
    "print(grid_rf_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2863028b-4dab-4ea8-b94f-3d86fb071c3f",
   "metadata": {},
   "source": [
    "Fitting a model via GridSearchCV returns: \n",
    "- A results log (*.cv_results_*)\n",
    "- The best results (*.best_index_*, *.best_params_* and *.best_score_*)\n",
    "- Extra Information (*.scorer_*, *.n_splits_* and *.refit_time_*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0701f2-1a87-4192-83d7-51c98a22d244",
   "metadata": {},
   "source": [
    "# Random Search \n",
    "\n",
    "Very similar to grid search in terms of setup, but instead of training a model for every single combination of parameters, it randomly picks these combinations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a562b78-e68a-4135-a2e4-b1bd2268bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create a Random Forest Classifier with specified criterion\n",
    "rf_class = RandomForestClassifier(criterion='entropy')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']} \n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "rand_rf_class = RandomizedSearchCV(\n",
    "    estimator=rf_class,\n",
    "    param_distributions=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    cv=5,\n",
    "    refit=True, \n",
    "    return_train_score=True, \n",
    "    n_iter=50\n",
    ")\n",
    "print(rand_rf_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bafc0d-6ef2-4f23-a8d1-0a548d6aa437",
   "metadata": {},
   "source": [
    "# Informed Search \n",
    "\n",
    "An informed search performs an initial random search to evaluate the best regions to perform a grid search\n",
    "\n",
    "## Bayesian Hyperparameter Tuning\n",
    "\n",
    "It uses bayes theory updating beliefs using evidence on model performance.\n",
    "\n",
    "The library [Hyperopt](https://hyperopt.github.io/hyperopt/) implements it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6f3f4-81e9-48a4-8487-01da16a32ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt as hp\n",
    "\n",
    "# Set up space dictionary with specified hyperparameters\n",
    "space = {'max_depth': hp.('max_depth', 2, 10, 2),'learning_rate': hp.uniform('learning_rate', 0.001, 0.9)}\n",
    "\n",
    "# Set up objective function\n",
    "def objective(params):\n",
    "    params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}\n",
    "    gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) \n",
    "    best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()\n",
    "    loss = 1 - best_score\n",
    "    return loss\n",
    "\n",
    "# Run the algorithm\n",
    "best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.default_rng(42), algo=tpe.suggest)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a872aaa-4068-49db-94de-d069e463b731",
   "metadata": {},
   "source": [
    "## Genetic Algorithms\n",
    "\n",
    "[TPOT](https://epistasislab.github.io/tpot/) implements this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
