{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8f25e3-5a01-48c6-8545-6d1ac518736f",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "Model validation consists of: \n",
    "- ensuring your model performs as expected on new data\n",
    "- testing model performance on holdout datasets\n",
    "- selecting the best model, parameters and accuracy metrics\n",
    "- achiving the best accuracy for the data given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439cc99-0845-4dfb-8685-b83432120fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import warnings \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9b989-443c-47f3-94af-a2db35e93975",
   "metadata": {},
   "source": [
    "We refer to *seen data* as the data that has been used during the training phase and *unseen data* the one not used for trainig.\n",
    "\n",
    "We call *testing data* the one left aside to asses model performance. \n",
    "\n",
    "Often the ratio is 80% of the available data for training and the other 20% for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f1c9c-c0b4-4fb3-b982-44eb3f10412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt_df = pd.read_csv('../data/tic-tac-toe.csv')\n",
    "ttt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9b22a-96e0-4ccf-b03a-7bcbbb37de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(ttt_df.iloc[:, :9])\n",
    "y = ttt_df['Class'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5cf0d8-32ec-4093-a08d-0cff15387186",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08187cfd-e2d0-49f4-a1d9-cf40d4758603",
   "metadata": {},
   "source": [
    "If we want to test model parameters we need another kind of data other than the trainint or testing. We call this new kind of data *validation set*. \n",
    "\n",
    "For creating training, validation and test sets we can use the *.train_test_split()* sklearn method twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37328ac2-7099-42a9-91c0-4fc7e9302b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing datasets. Use 10% for the test set\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "# Create temporary training and final testing datasets\n",
    "X_temp, X_test, y_temp, y_test  =\\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=1111)\n",
    "\n",
    "# Create the final training and validation datasets\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X_temp, y_temp, test_size=0.25, random_state=1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846bc4c-fee5-4e6a-b554-b74420def1b4",
   "metadata": {},
   "source": [
    "# Accuracy Metrics\n",
    "\n",
    "Accuracy metrics are always application specific\n",
    "\n",
    "## Regression Models \n",
    "\n",
    "MAE and MSE error terms are in different units and should not be compared\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "- Simplest and most intuitive metric\n",
    "- Treats all points equally\n",
    "- Not sensitive to outliers\n",
    "\n",
    "$$MAE = \\frac{\\sum_{i=1}^{n} |y_i - \\hat{y_i}|}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e7e13-f024-4cae-80a3-523827c5a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13541201-913c-488b-9064-f46e84e0ac94",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)\n",
    "\n",
    "- Most widely used regression metric\n",
    "- Allow outlier errors to contribute more to the overall error\n",
    "\n",
    "$$MSE = \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259bbbc9-24b2-4830-9786-3cd75ce2c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# mean_squared_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26ee32-7a04-4be6-900b-7b7129ee7d27",
   "metadata": {},
   "source": [
    "Sometimes we are interested in knowing how well our model performs on a particular subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf480c6-c6bf-49a2-9a68-f88217c12ca0",
   "metadata": {},
   "source": [
    "## Classification Models \n",
    "\n",
    "There are several accuracy metrics for classification models: precision, recall, accuracy, f1 score...\n",
    "They all can be easily calculated from the confusion matrix: \n",
    "\n",
    "|                | Predicted 0  | Predicted 1  |\n",
    "|----------------|--------------|--------------|\n",
    "| **Actual 0**   | 23 (TN)      | 7 (FP)       |\n",
    "| **Actual 1**   | 8 (FN)       | 62 (TP)      |\n",
    "\n",
    "- **True Positive (TP)**: Predict/Actual are both 1\n",
    "- **True Negative (TN)**: Predict/Actual are both 0\n",
    "- **False Positive (FP)**: Predicted 1, actual 0\n",
    "- **False Negative (FN)**: Predicted 0, actual 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1bd68-0306-4cb7-bfd2-1f22a2189c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6798fa4-4f1d-4c49-a34e-9ed35bd3e2df",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "- Represents the hability of our model to correctly predict the correct classification\n",
    "  \n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e12292-235a-4fb3-adf4-f621e0be5669",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "- Used when we dont want to overpredict the positive class\n",
    "  \n",
    "$$Precision = \\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fc6b0-f92c-4cbb-9a6e-0f3c311ebd68",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "- Its about finding all the positive values\n",
    "- Used when we cant afford to lose any positive values.\n",
    "  \n",
    "$$Recall = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1f325b-d7b2-441b-9e53-e2c662916b22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T08:31:18.269273Z",
     "iopub.status.busy": "2024-09-16T08:31:18.260605Z",
     "iopub.status.idle": "2024-09-16T08:31:18.276959Z",
     "shell.execute_reply": "2024-09-16T08:31:18.275785Z",
     "shell.execute_reply.started": "2024-09-16T08:31:18.268348Z"
    }
   },
   "source": [
    "### F1 Score\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall, providing a single metric that balances the trade-off between the two. \n",
    "\n",
    "It is particularly useful when the dataset is imbalanced, meaning one class is significantly more common than the other. In such cases, relying solely on accuracy can be misleading, as it may mask poor performance on the minority class. \n",
    "\n",
    "The F1 score helps to account for both false positives and false negatives, making it ideal when both types of errors are important.\n",
    "\n",
    "$$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911a957-43a6-432b-8efb-c9785b8ddd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy_score(y_test, test_predictions)\n",
    "precision_score(y_test, test_predictions)\n",
    "recall_score(y_test, test_predictions)\n",
    "f1_score(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a20b7-4041-442c-87cc-01819c7c8be7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T08:38:50.596759Z",
     "iopub.status.busy": "2024-09-16T08:38:50.595304Z",
     "iopub.status.idle": "2024-09-16T08:38:50.606434Z",
     "shell.execute_reply": "2024-09-16T08:38:50.604827Z",
     "shell.execute_reply.started": "2024-09-16T08:38:50.596679Z"
    }
   },
   "source": [
    "# The Bias-Variance tradeoff\n",
    "\n",
    "## Variance\n",
    "\n",
    "- Variance occurs when the model pays too much attention to the training data and fails to generalize.\n",
    "- Low training error but high testing error\n",
    "- Occurs when models are overfit and have high complexity\n",
    "- **Overfit** happens when out model starts to attach meaning to the noise of our data.\n",
    "- You can spot overfit because the training error would be much lower than the test error.\n",
    "\n",
    "## Bias \n",
    "\n",
    "- Failing to find the relationship between the data and the response\n",
    "- Leads to high training and test error\n",
    "- Occurs when models are underfit\n",
    "- Underfitting occurs when the model cannot find patterns in the data\n",
    "- Underfitting is difficult to spot since both training and test errors are high.\n",
    "\n",
    "# Cross Validation\n",
    "\n",
    "Models and accuracies can be very dependent on the data in each of the sets when using holdout sets.\n",
    "\n",
    "Cross validation helps mitigating the split dependency of the holdout approach.\n",
    "\n",
    "Cross validations divides the training data into n folds and perform a training using n-1 folds and validates on the last fold. It runs training and validation n times, rotating the validation set.\n",
    "\n",
    "When using cross validation we often report the mean of the errors as the overall error. We can calculate the std as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486562ff-e755-4f0a-a3e1-b0bc00eda512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "candy_data = pd.read_csv('../data/candy-data.csv')\n",
    "X=candy_data.iloc[:,1:11]\n",
    "y=candy_data[['winpercent']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd570b9-b756-45c7-9c0e-bb06cb976e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestRegressor(n_estimators=25, random_state=1111)\n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "# Set up cross_val_score\n",
    "cv = cross_val_score(estimator=rfc,\n",
    "                     X=X_train,\n",
    "                     y=y_train,\n",
    "                     cv=10,\n",
    "                     scoring=mse)\n",
    "\n",
    "# Print the mean error\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe580cd4-6cbb-400f-bba1-54193d67784d",
   "metadata": {},
   "source": [
    "# Leave One Out Cross Validation (LoOCV)\n",
    "\n",
    "CV with the number of splits equals to the number of observations, so we train on all the data but one point and we test on that point.\n",
    "\n",
    "We can use it when the data is limited.\n",
    "\n",
    "Gives the best error estimate possible.\n",
    "\n",
    "Its computational expensive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b65c4-94bd-4aa2-8d67-37f968d0a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "# Create scorer\n",
    "mae_scorer = make_scorer(mean_absolute_error)\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=15, random_state=1111)\n",
    "\n",
    "# Implement LOOCV\n",
    "scores = cross_val_score(rfr, X=X, y=y, cv=X.shape[0], scoring=mae_scorer)\n",
    "\n",
    "# Print the mean and standard deviation\n",
    "print(\"The mean of the errors is: %s.\" % np.mean(scores))\n",
    "print(\"The standard deviation of the errors is: %s.\" % np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ded78-6b1a-41ce-a0a4-c3fad29d3a48",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
