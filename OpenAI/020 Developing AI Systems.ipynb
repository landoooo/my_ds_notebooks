{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18088d60-de6c-43e7-873f-982daa18fcd5",
   "metadata": {},
   "source": [
    "# Structuring an API call \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37068b8-c713-4d47-bfbc-ea4fec60bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os \n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5001748-d24f-4dc2-8872-1fe2cc990535",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages =[{\n",
    "        'role': 'user', \n",
    "        'content': 'Who developed chatgpt?'\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825a484-1fcf-4271-91e6-7250e7d36148",
   "metadata": {},
   "source": [
    "# Challenges on a production environment\n",
    "\n",
    "## Error Handling \n",
    "- Display useful error messages\n",
    "- Alternatives for when the service is unavailable\n",
    "\n",
    "## Moderation and Safety \n",
    "- Control unwanted inputs\n",
    "- Minimizing the risk of data leaks\n",
    "\n",
    "## Testing and validation \n",
    "- Checking for responses that are out of topic\n",
    "- Testing for inconsisten behavior\n",
    "\n",
    "## Communication with External Systems \n",
    "- Calling external functions and APIs\n",
    "- Optimizing response times\n",
    "\n",
    "# Specific Output Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5115633-2135-4658-9e89-8296046a0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages =[{\n",
    "        'role': 'user', \n",
    "        'content': 'Give me the best 3 novels of all time in json format'\n",
    "    }]\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a13bd-fdbf-44f7-ba4f-5356bd2bca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the request\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "   {\"role\": \"user\", \"content\": \"Give me the 3 best graphic novels of all time in json format\"}\n",
    "  ],\n",
    "  # Specify the response format\n",
    "  response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bdd91-cfb4-4cb5-b595-b52d460a655f",
   "metadata": {},
   "source": [
    "# Handling errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc702dcd-17d0-4f1c-af03-be2b91179818",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='text-davinci-001', \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"give me the 3 best post rock albums of all time in json format\"\n",
    "        }\n",
    "   ],\n",
    "    response_format='json_object'\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].content.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fd7cf-988c-44fe-b0e6-b53c4885da6c",
   "metadata": {},
   "source": [
    "## Connection Errors\n",
    "Common expectable errors on either server or client side are **InternalServerError**, **APIConnectionError** or **APITimeourError**\n",
    "\n",
    "In these cases: \n",
    "- Check connection configuration\n",
    "- Reach out to support\n",
    "\n",
    "## Resource Limits Errors \n",
    "**ConflictError** and **RateLimitError** \n",
    "\n",
    "- Check limit restrictions\n",
    "- Ensure requests are within limits\n",
    "\n",
    "## Authentication errors \n",
    "Wrong, expired or revoked api keys\n",
    "\n",
    "## Bad Request errors\n",
    "Check the expected request documentation for missing keys or typos.\n",
    "\n",
    "\n",
    "# Handling Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67996d94-6792-41c1-ad63-8bd7c1285285",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages= [{\n",
    "            'role': 'user',\n",
    "            'content': 'list five data science professions'\n",
    "        }]\n",
    "    )\n",
    "except openai.AuthenticationError as e:\n",
    "    print(f'OpenAI failed to authenticate: {e}')\n",
    "    pass\n",
    "except openai.RateLimitError as e:\n",
    "    print(f'OpenAI API request exceeded rate limit: {e}')\n",
    "    pass\n",
    "except Exception as e: \n",
    "    print(f'Unable to generate a response. Exception: {e}')\n",
    "    pass \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720dec28-5bac-44d1-b5c3-c0e8c38cb658",
   "metadata": {},
   "source": [
    "# RateErrorLimit \n",
    "\n",
    "The RateLimitError could be caused by: \n",
    "- Too many requests in a short period of time\n",
    "- Too many tokens in the request\n",
    "\n",
    "## Solutions \n",
    "\n",
    "### Retry (short wait between the requests)\n",
    "Retry can be achieved with the python's tenacity library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fcf746-92e6-4270-b99f-6777045a1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import (retry, stop_after_attempt, wait_random_exponential)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(2))\n",
    "def get_response():\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages= [{\n",
    "            'role': 'user',\n",
    "            'content': 'list five data science professions'\n",
    "        }]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "get_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b165dd4-c76d-47b0-9161-f9e6938dacbf",
   "metadata": {},
   "source": [
    "### Batching (Processing multiple messages in one request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e212025-f8df-4ab1-b91c-ebf4eced5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ['Pearl Jam', 'Radiohead', 'Sigur Ros']\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        'role': 'system', \n",
    "        'content': '''You are given a series of rock bands and are asked to return \n",
    "        their best album along with the publication year. Provide each answer in the \n",
    "        response as a separate content''',\n",
    "    }\n",
    "]\n",
    "[message.append({'role': 'user', 'content': band}) for band in bands]\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be1b18-a728-4ee8-94aa-9eb861a1d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o-mini', \n",
    "    messages=message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d37180-b40a-41fb-b436-e808b2bce73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e672a-0a82-45dc-94fa-28f026dab390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T14:15:20.072963Z",
     "iopub.status.busy": "2024-11-21T14:15:20.072360Z",
     "iopub.status.idle": "2024-11-21T14:15:20.081894Z",
     "shell.execute_reply": "2024-11-21T14:15:20.080564Z",
     "shell.execute_reply.started": "2024-11-21T14:15:20.072930Z"
    }
   },
   "source": [
    "### Reducing the amount of tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87840a-2e95-400d-90c6-df0c376ebb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken \n",
    "\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "prompt = 'tokens can be full words or groups of characters commonly grouped together: tokenization'\n",
    "\n",
    "encoded = encoding.encode(prompt)\n",
    "num_tokens = len(encoded)\n",
    "print(f\"Num tokens: {num_tokens}\")\n",
    "print(f\"encoded: {encoded[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f327771-1f75-470f-8605-1f5ba48a0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = {\"role\": \"user\", \"content\": \"I'd like to buy a shirt and a jacket. Can you suggest two color pairings for these items?\"}\n",
    "\n",
    "# Use tiktoken to create the encoding for your model\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "# Check for the number of tokens\n",
    "num_tokens = len(encoding.encode(input_message['content']))\n",
    "\n",
    "# Run the chat completions function and print the response\n",
    "if num_tokens <= 100:\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini\", messages=[input_message])\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"Message exceeds token limit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5e9bc-dd5b-43c9-b723-eefa01cdcaeb",
   "metadata": {},
   "source": [
    "# Defining Function Calling \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
