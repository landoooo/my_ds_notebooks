{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e715c-e5b9-4cef-a1c2-4b4cd66e2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154f177-0df5-4ed3-b011-c0c102591ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    # Write your prompt\n",
    "    prompt=\"In two sentences, how can the OpenAI API be used to upskill myself?\",\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc689ba-707b-494d-9e15-128c4d109bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the response into a dictionary\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2d9ba-8f43-4ee9-b83e-66a00caec42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an organization can help with limits, billing, etc. \n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  organization='org-4otiQhcm1LBuWdDYRBImSUlI',\n",
    "  project='proj_bAW2JMzoEkGe09EcZZrTQ5oi',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f5219-988e-4ba8-b6ad-6442ed24185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d256dc-2ac4-4057-bcdc-173f89ce066d",
   "metadata": {},
   "source": [
    "# OpenAI Models\n",
    "\n",
    "## Completions \n",
    "\n",
    "- Receive continuation of a prompt\n",
    "- Single Turn Tasks\n",
    "    - Answer quesions\n",
    "    - Classification\n",
    "    - Sentiment Analysis\n",
    "    - Explain Complex Topics\n",
    "\n",
    "When we send a text to a text completion endpoint, the api responds with the text that is *most likely* to complete the prompt. Responses are non-deterministic, though.\n",
    "\n",
    "Sometimes this randomness is not desirable, like in a customer service chatbot. **Temperature** is a parameter ranging from 0 (highly deterministic) to 2 (very random). \n",
    "\n",
    "On top of answering questions, other kinds of tasks where the completions model can help are summarization, text content generation and transformation (find and replace)...\n",
    "\n",
    "The **max_tokens** param helps shape the size of the response and control the price.\n",
    "\n",
    "Classification of sentences or tokens is another capability of the completions model.\n",
    "\n",
    "Depending on the amount of examples provided, our requests can be categorized as:\n",
    "- **Zero shot**: no example\n",
    "\n",
    "These two are colled in-context learning:\n",
    "- **One shot**: one example\n",
    "- **Few shot**: several examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c4a89-fee4-4091-874b-18a023c8582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for temperature in [0,1]:\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        # Write your prompt\n",
    "        prompt=\"In two sentences, how can the OpenAI API be used to upskill myself?\",\n",
    "        max_tokens=100, \n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    print(f\"Temperature {temperature}: {response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca306a8-dc87-484b-93df-4b7ce26efce2",
   "metadata": {},
   "source": [
    "## Chat \n",
    "\n",
    "- Multi-turn conversations\n",
    "  - Ideation\n",
    "  - Customer Support Assistant\n",
    "  - Personal Tutor\n",
    "  - Translations\n",
    "  - Write code\n",
    "- Also performs well single turn tasks\n",
    "\n",
    "One benefit of the chat completions is that they offer better customization through the use of *roles*\n",
    "\n",
    "Another benefit is the instruct models are more expensive than the chat ones.\n",
    "\n",
    "### Roles \n",
    "\n",
    "- **System**: controls assistant's behavior\n",
    "- **User**: instructs the assistant\n",
    "- **Assistant**: response to the user instruction.\n",
    "  - Can also be written by the user to provide examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5080bde-0d3e-48e0-b73c-d1379cbe1b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"you are a data science tutor who speaks concisely\"}, \n",
    "              {\"role\": \"user\", \"content\": \"what is the difference between lasso and ridge\"}, \n",
    "             ],\n",
    "    max_tokens=200,\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75903705-02c7-45e2-b395-6aab3667a121",
   "metadata": {},
   "source": [
    "We can provide example messages as a conversation to the system so it can learn how do we want it to interact with us.\n",
    "\n",
    "Storing responses allow us to create back and forth conversations. Thats how chatgpt works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b9226-b396-41df-b55a-9df8fc0585cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "   model=\"gpt-4o-mini\",\n",
    "   # Add a user and assistant message for in-context learning\n",
    "   messages=[\n",
    "     {\"role\": \"system\", \"content\": \"You are a helpful Python programming tutor.\"},\n",
    "     {\"role\": \"user\", \"content\": \"Explain python lists\"},\n",
    "     {\"role\": \"assistant\", \"content\": \"Python list is a type of python ordered structure that can contain other objects.\"},\n",
    "     {\"role\": \"user\", \"content\": \"Explain what the type() function does.\"}\n",
    "   ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347f04eb-f5ab-471b-b690-dbddf729fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}]\n",
    "user_msgs = [\"Explain what pi is.\", \"Summarize this in two bullet points.\"]\n",
    "\n",
    "for q in user_msgs:\n",
    "    print(\"User: \", q)\n",
    "    \n",
    "    # Create a dictionary for the user message from q and append to messages\n",
    "    user_dict = {\"role\": \"user\", \"content\": q}\n",
    "    messages.append(user_dict)\n",
    "    \n",
    "    # Create the API request\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Convert the assistant's message to a dict and append to messages\n",
    "    assistant_dict = {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "    messages.append(assistant_dict)\n",
    "    print(\"Assistant: \", response.choices[0].message.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706cc02-7a94-4cd2-b0ea-279a1f3f5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial system message to set the assistant's behavior\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"you are an experienced tourist guide with deep knowledge about Paris\"}\n",
    "]\n",
    "\n",
    "# Function to add new messages to the conversation and get a response\n",
    "def send_message(conversation, user_input):\n",
    "    # Add user's message to the conversation\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Call the GPT-3.5 Turbo model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Use the GPT-3.5 Turbo model\n",
    "        messages=conversation,  # Pass the conversation history\n",
    "        temperature=0,          # Set temperature for deterministic output\n",
    "        max_tokens=100          # Limit the response to 100 tokens\n",
    "    )\n",
    "\n",
    "    # Get assistant's response\n",
    "    assistant_reply = response.choices[0].message.content\n",
    "    \n",
    "    # Add assistant's response to the conversation\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "\n",
    "    # Print the assistant's reply\n",
    "    print(assistant_reply)\n",
    "\n",
    "# List of user inputs\n",
    "user_inputs = [\n",
    "    \"How far away is the Louvre from the Eiffel Tower (in miles) if you are driving?\",\n",
    "    \"Where is the Arc de Triomphe?\",\n",
    "    \"What are the must-see artworks at the Louvre Museum?\"\n",
    "]\n",
    "\n",
    "# Send each message one by one\n",
    "for user_input in user_inputs:\n",
    "    send_message(conversation, user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f7173-4d94-4887-a5c3-47ba27126132",
   "metadata": {},
   "source": [
    "## Moderation \n",
    "\n",
    "- Checks content for violations of OpenAIs usage policies, including inciting violence or hate speech.\n",
    "- The sensitivity can be customized\n",
    "\n",
    "The categoryScores object of the response of the moderation model gives us fine details about different aspects of the moderation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da03bae-a1e8-425b-9170-7a0f741ed1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "# Create a request to the Moderation endpoint\n",
    "response = client.moderations.create(\n",
    "    model='text-moderation-latest',\n",
    "    input='My favorite book is To Kill a Mockingbird.'\n",
    ")\n",
    "\n",
    "# Print the category scores\n",
    "print(response.results[0].category_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e7749-88e8-419d-8aa9-e4517cffc4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(response.results[0].category_scores, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cad134-34ec-4373-be3c-82ff85cdd6c9",
   "metadata": {},
   "source": [
    "# Speech to Text Transcription with Whisper \n",
    "\n",
    "Speech to text capabilities: \n",
    "- Transcribe audio\n",
    "- Translate and transcribe audio into English\n",
    "\n",
    "The quality of the translation depends on the audio quality, the audio language and model's knowledge of the subject matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd725f9-5ee3-40c7-8070-aa76399f7a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = open('11 oct 9.53_.mp3', 'rb')\n",
    "\n",
    "# Create a request to the Moderation endpoint\n",
    "response = client.audio.transcriptions.create(\n",
    "    model='whisper-1',\n",
    "    file=audio\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b9cba-50bf-44a8-8d64-6211ceb429b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = open('flas-christmas-sp.mp3', 'rb')\n",
    "\n",
    "# Create a request to the Moderation endpoint\n",
    "response = client.audio.translations.create(\n",
    "    model='whisper-1',\n",
    "    file=audio\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86dd7a0-f11d-479a-806f-dc88d0c06ed4",
   "metadata": {},
   "source": [
    "In order to help the quality of the transcription we can provide an optional prompt indicating the style of the output we expect and context for the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a8d73-4257-41f9-ad9c-4e6a2ced8766",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = open('flas-christmas-sp.mp3', 'rb')\n",
    "\n",
    "# Create a request to the Moderation endpoint\n",
    "response = client.audio.translations.create(\n",
    "    model='whisper-1',\n",
    "    file=audio,\n",
    "    prompt=\"[Music] In Spain, the whole family gathers on Dec. 24th to celebrate Christmas Eve.\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96318363-725f-49d7-984e-f8dbfff349bf",
   "metadata": {},
   "source": [
    "# Combining Models\n",
    "\n",
    "Chaining models is feeding the output from one model into another \n",
    "Can use the same model multiple times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361680ba-4f11-4703-b0bb-946b00525454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
