{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc2e5d9-27f8-4f01-b5f6-006c9d13af1d",
   "metadata": {},
   "source": [
    "# Experimental Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f580b35-c7ca-43be-b101-194dcbe72f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b005c62-99a7-4c42-bd2c-e213eaeb8509",
   "metadata": {},
   "source": [
    "Experimental design is the process in which we carry out research in an objective and controlled fashion. The purpose of this is to ensure we can make specific conclusions in reference to a hypothesis we have.\n",
    "\n",
    "https://www.sciencedirect.com/topics/earth-and-planetary-sciences/experimental-design\n",
    "\n",
    "Because we use objective tools, we need to use quantified language. Instead of using words like 'probably', 'likely', and 'small' when noting our conclusions, we should use precise and quantified language. This often takes the form of noting the percentage risk on a Type I error in the conclusion. \n",
    "\n",
    "Type I errors: we incorrectly reject the null hypothesis when it is actually true. \n",
    "\n",
    "## Terminology\n",
    "\n",
    "**Subjects**\n",
    ": What we are experimenting on\n",
    "\n",
    "**Treatment**\n",
    ": Some change given to one group\n",
    "\n",
    "**Control group**\n",
    ": The group not given any treatment\n",
    "\n",
    "## How to assign subjects to groups\n",
    "\n",
    "- Non Random (iloc with a range for instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe498b-9547-4895-bcd5-a3e0517ca069",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather('../data/dem_votes_potus_12_16.feather')\n",
    "\n",
    "group1 = df.iloc[0:100]\n",
    "group2 = df.iloc[100:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843bf591-c52a-4432-8900-41af0c1295c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413b773-b7d7-4c47-a684-a5a4b1303673",
   "metadata": {},
   "outputs": [],
   "source": [
    "group2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc665d-f7cd-485f-af7d-79588eae0aae",
   "metadata": {},
   "source": [
    "- Random (sample method for instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4a312-f3fc-4e43-9d20-f40b3101809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_group1 = df.sample(frac=0.5)\n",
    "random_group1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe35d41-5685-4832-8cac-9ca743be65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_group2 = df.drop(random_group1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b08231-4a99-49de-ae7d-9d9e0e879c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df_rand = pd.concat([random_group1['dem_percent_12'].describe(), random_group2['dem_percent_12'].describe()], axis=1)\n",
    "compare_df_rand.columns = ['group1', 'group2']\n",
    "\n",
    "print(compare_df_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f8515-c724-49a7-8f34-a8c8834a8738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T15:16:17.811886Z",
     "iopub.status.busy": "2024-08-03T15:16:17.811087Z",
     "iopub.status.idle": "2024-08-03T15:16:17.822205Z",
     "shell.execute_reply": "2024-08-03T15:16:17.820904Z",
     "shell.execute_reply.started": "2024-08-03T15:16:17.811840Z"
    }
   },
   "source": [
    "# Experimental data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf92bd-2137-4787-815c-86c8c1644e8c",
   "metadata": {},
   "source": [
    "Randomization is often the best technique for setting up experimental data, but it isnt always.\n",
    "\n",
    "## Scenarios where randomization could cause undesiderable outcomes\n",
    "\n",
    "### Uneven issue\n",
    "\n",
    "Different number of subjects in groups. Can be solved with block randomization.\n",
    "\n",
    "### Covariates\n",
    "\n",
    "Covariates are variables that potentially affect experiment results but aren't the primary focus. If covariates are highly variable or not equally distributed among groups, randomization might not produce balanced groups. This imbalance can lead to biased results. Overall these make it harder to see an effect from a treatment, as these issues may be driving an observed change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd6c8e9-d2cd-41a2-9556-a99154327d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = df.sample(frac=0.5, replace=False)\n",
    "group1['Block']=1\n",
    "\n",
    "group2 = df.drop(group1.index)\n",
    "group2['Block']=2\n",
    "\n",
    "print(len(group1), len(group2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d11c0-fbc9-4295-8794-47bd4e203d4c",
   "metadata": {},
   "source": [
    "But does this technique eliminates the covariate issue?\n",
    "\n",
    "A nice way of checking for potential covariate issues is with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359d3a8-26b6-4e88-93f0-ee797db000d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='dem_percent_12', fill=True, kind='kde'\n",
    "            # , hue=''\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5427d-94af-4d4e-b85f-0b058f5a7a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T15:57:13.085219Z",
     "iopub.status.busy": "2024-08-03T15:57:13.084292Z",
     "iopub.status.idle": "2024-08-03T15:57:13.098714Z",
     "shell.execute_reply": "2024-08-03T15:57:13.096956Z",
     "shell.execute_reply.started": "2024-08-03T15:57:13.085172Z"
    }
   },
   "source": [
    "Not with this dataset, but it could happen that based on a second feature, there is quite a difference in the group distributions. When an effect could be because of a variable rather than the treatment, this is often called **confounding**. The covariate issue can be solved with stratified randomization.\n",
    "\n",
    "### Stratified randomization\n",
    "\n",
    "Stratified randomization involves splitting based on a potentially confounding variable first, followed by randomization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a4c21-a13c-44e1-8970-90dfe1cf7d14",
   "metadata": {},
   "source": [
    "# Normal Data\n",
    "\n",
    "Normal data is drawn from a normal distribution\n",
    "\n",
    "The normal distribution is related to z-scores\n",
    "\n",
    "$$z = \\frac{ x - \\mu}{ \\sigma }$$\n",
    "\n",
    "The most common normal distribution is the standard one, having $\\mu$=0 and $\\sigma$=0\n",
    "\n",
    "The normal distribution is behind many of the statistical **parametric** tests. There are also# **non parametric** tests that dont assume normal data.\n",
    "\n",
    "To visually check if a dataset follow a normal distribution we can plot the kde\n",
    "\n",
    "Another visual tool are the qqplots, that compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16efeb58-6bb3-4282-a564-fda1ae3c8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/chick_weight.csv')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a0f2f-fa62-49d3-afda-c8d075130992",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows=2\n",
    "n_cols=2\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols)\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    sns.distplot(df[column],ax=axes[i//n_cols,i%n_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738bc7d-7d92-4d3d-b500-c914b3f039cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from scipy.stats.distributions import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df74626-3efe-46db-a46c-4464345be8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(data=df.weight, line='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4102bb-aa69-4ca6-b4c9-796b212dd876",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(df['weight'], \n",
    "       line='s', \n",
    "       dist=norm\n",
    "      ) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2dfc6-79cb-4fda-b8fd-ce9168732e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data\n",
    "subset_data = df[df['Time'] == 2]\n",
    "\n",
    "# Repeat the plotting\n",
    "sns.displot(data=df, x='weight', kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd7e9e-5c66-4519-9bbb-88121d0fa90a",
   "metadata": {},
   "source": [
    "- ideally, the dots should follow the line\n",
    "- bad: bow out at the ends\n",
    "\n",
    "Other tests for normality: \n",
    "- Shapiro-Wilk: good for smaller datasets\n",
    "- D'Agostino $K^2$ (uses curtosis and skewness\n",
    "- Anderson-Darling returns a list of values\n",
    "\n",
    "For all these, the $H<sub>0</sub>$ is \"data is drawn from a Normal distribution\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48433ed7-19ba-479a-b247-bdd7243f33ff",
   "metadata": {},
   "source": [
    "# A Shapiro Wilk test\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "alpha = 0.05\n",
    "stat, p = shapiro(df.weight)\n",
    "print(f'p:{round(p,4)} test stat: {round(stat, 4)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f8614-a424-42c0-b0dc-95a4b2f141f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Anderson Darling test\n",
    "from scipy.stats import anderson\n",
    "\n",
    "alpha = 0.05\n",
    "result = anderson(x = df.weight, dist=\"norm\")\n",
    "print(round(result.statistic,4))\n",
    "print(result.significance_level)\n",
    "print(result.critical_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a317d4-8251-42b8-bac4-133fc83c7084",
   "metadata": {},
   "source": [
    "# Factorial Design\n",
    "\n",
    "Factorial designs study multiple independent variables/factors in one experiment\n",
    "\n",
    "They test every combination of factor levels. It discovers direct effects and interactions between factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a2f7e0-ba47-4843-be1e-a1623b0d32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_data = pd.read_feather('../data/marketing_data.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc36da4b-fde1-4e9f-bedc-ad0f3b5b73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table for marketing campaign data\n",
    "marketing_pivot = marketing_data.pivot_table(\n",
    "  values='Conversions', \n",
    "  index='Messaging_Style', \n",
    "  columns='Time_of_Day', \n",
    "  aggfunc='mean')\n",
    "\n",
    "# View the pivoted results\n",
    "print(marketing_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240b55f-5c12-4046-b9c1-657b05b464c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interactions with a heatmap\n",
    "sns.heatmap(marketing_pivot, \n",
    "         annot=True, \n",
    "         cmap='coolwarm',\n",
    "         fmt='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a409b18-abb0-4acf-97eb-e75baabcbc6c",
   "metadata": {},
   "source": [
    "The biggest change is registered in the evening messages from casual to formal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced64364-6497-4985-9921-8461e5b065c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T07:18:02.181075Z",
     "iopub.status.busy": "2024-08-04T07:18:02.179977Z",
     "iopub.status.idle": "2024-08-04T07:18:02.193060Z",
     "shell.execute_reply": "2024-08-04T07:18:02.191621Z",
     "shell.execute_reply.started": "2024-08-04T07:18:02.180995Z"
    }
   },
   "source": [
    "### Factorial designs\n",
    "- Multiple treatments and interactions\n",
    "- Dissect complex multi variable effects and interactions\n",
    "- Can require more subjects\n",
    "\n",
    "### Block Designs\n",
    "- Group similar subjects in randomized designs\n",
    "- Control within-block variance\n",
    "- Each treatment is tested within every block\n",
    "\n",
    "# Randomized block design\n",
    "\n",
    "Blocking involves grouping experimental units, often with similar characteristics, to minimize variance within these groups\n",
    "Each block, representing a specific level of the blocking factor, receives every treatment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73f0c7-5b72-43b1-b46d-2e63e4ef7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes = pd.read_csv('../data/athletic_perf.csv')\n",
    "athletes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb848e-3e96-4e80-9e54-ab2b4daa520c",
   "metadata": {},
   "source": [
    "To implement a randomized block design, we'll group the rows into blocks based on an initial feature, shuffle the rows within these blocks, and randomly assign a treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3b62b-fd08-41ae-9404-ff659986750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = athletes.groupby('Initial_Fitness').apply(\n",
    "    # Lets shuffle each block\n",
    "    lambda x:x.sample(frac=1)\n",
    ")\n",
    "blocks = blocks.reset_index(drop=True)\n",
    "blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338aa7ec-bc59-4c14-86c0-605f46b631b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, within each block, we assign exercise program randomly\n",
    "blocks['Treatment'] = np.random.choice(\n",
    "    ['Cardio', 'Strength Training', 'Mixed'], \n",
    "    size=len(blocks)\n",
    ")\n",
    "\n",
    "blocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ba328-7d6c-4799-b94b-4ac8870d2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A boxplot is an effective tool for visualizing the distribution of treatment effects across different blocks \n",
    "\n",
    "sns.boxplot(x='Initial_Fitness', y='Performance_Inc', hue='Treatment', data=blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3ff47-5ca1-4a90-8c18-44a7752ae14d",
   "metadata": {},
   "source": [
    "We can use ANOVA to statistically check for these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e32115-e505-4684-a8d8-70212f843be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.05\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "blocks.groupby(\"Initial_Fitness\").apply(\n",
    "    lambda x: f_oneway(x[x['Treatment']== 'Cardio']['Performance_Inc'],\n",
    "                       x[x['Treatment']== 'Mixed']['Performance_Inc'], \n",
    "                       x[x['Treatment']== 'Strength Training']['Performance_Inc'], \n",
    "                      )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce7c977-4915-432b-96ed-cb42c01a7984",
   "metadata": {},
   "source": [
    "# Covariate adjustment in experimental design\n",
    "\n",
    "Covariates are variables that are not of primary interest but are related to the outcome variable and can influence its analysis. Including covariates in statistical analyses is crucial for reducing confounding, which occurs when an external variable influences both the dependent variable and independent variable(s).\n",
    "\n",
    "By adjusting for covariates researchers can isolate the effect of the independent variable on the outcome, minimizing the influence of confounders.\n",
    "\n",
    "Like that, the analysis better reflects the true effect by isolating it from the influence of covariates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1817e0-0f7f-4692-80fd-d4c41a0bf4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_chick_data = pd.read_csv('../data/chick_weight.csv')\n",
    "cov_chick_data = pd.read_csv('../data/chick_cov.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a6225-b350-4a83-9062-7b8ef01c8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_chick_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa68ea7-379e-4720-8206-406dab00b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ANCOVA with Diet and Time as predictors\n",
    "model = ols('weight ~ Diet + Time', data=exp_chick_data).fit()\n",
    "\n",
    "# Print a summary of the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833133c-c417-4e64-a65e-fc3dd60255bb",
   "metadata": {},
   "source": [
    "Prob (F-statistic): The first portion of summary output provides details on the significance of the model; it shows a very small p-value, supporting for covariates affecting the model.\n",
    "Large p-values would implie a lack of support for covariates affecting the model.\n",
    "\n",
    "P>|t| of the second portion are very small, sugesting that each of them alone are significant predictors of growth for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291b3e6-9774-46d4-aed3-8f0d091a67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Diet effects with Time adjustment\n",
    "sns.lmplot(x='Time', y='weight', \n",
    "         hue='Diet', \n",
    "         data=exp_chick_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11514b-c440-437a-bd35-dba36775485b",
   "metadata": {},
   "source": [
    "The crossing lines here suggest we may add a The crossing regression lines suggest we may want to add an interaction term of time by Diet in another model. Parallel lines would suggest a lack of interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b579cdd-b3aa-4a35-b02c-64d9f0984f41",
   "metadata": {},
   "source": [
    "# Choosing the right statistical test\n",
    "\n",
    "Prior to choosing the right test, we have to pay attention to:\n",
    "- Dataset Features:\n",
    "  - Data types\n",
    "  - Distributions: often assumed to be normal in many tests\n",
    "  - Number of variables\n",
    "- Hypothesis\n",
    "\n",
    "Result: Accurate and dependable conclusions\n",
    "\n",
    "Lets see how to apply t-tests, ANOVA and $\\chi^2$\n",
    "\n",
    "The athletic performance dataset contains the impact of training programs and diet on athletic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f8449-2bc3-425b-8f22-bd5c0373b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "athletic_perf = pd.read_csv('../data/athletic_perf.csv')\n",
    "athletic_perf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e65a3e-7b5e-42b7-a10c-70e6e4df909f",
   "metadata": {},
   "source": [
    "## Independent Samples t-test\n",
    "\n",
    "Compares the means of two groups.\n",
    "It assumes normal distributions and equal variances to ensure the reliability and validity of the test results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6afe7-67fc-44f9-a1e0-ecd5c9a39b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "group1 = athletic_perf[athletic_perf.Training_Program == 'HIIT']['Performance_Inc']\n",
    "group2 = athletic_perf[athletic_perf.Training_Program == 'Endurance']['Performance_Inc']\n",
    "\n",
    "t_stat, p_val = ttest_ind(group1, group2) \n",
    "print (f't_stat: {t_stat}, p-value: {p_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df3309-0b19-4d40-a607-15722eecd50d",
   "metadata": {},
   "source": [
    "> p_val > $\\alpha$ -> insufficient evidence of a difference in means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2aa775-3cee-408d-bde2-3a3619d20fe0",
   "metadata": {},
   "source": [
    "## One Way ANOVA\n",
    "\n",
    "Compares the means across multiple groups (more than 2)\n",
    "\n",
    "Assumptions: equal variances among the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c3d64-5f8a-4ad8-aa2b-c4787255ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "program_types = ['HIIT', 'Endurance', 'Strength']\n",
    "\n",
    "groups = [athletic_perf[athletic_perf['Training_Program'] == program]['Performance_Inc']\n",
    "          for program in program_types]\n",
    "\n",
    "f_stat, p_val = f_oneway(*groups)\n",
    "\n",
    "print(f'F-statistic: {f_stat}, p_val: {p_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5d236-cc0d-41f7-b63c-ce0fbd2b9bc2",
   "metadata": {},
   "source": [
    "> p_val > $\\alpha$ -> insufficient evidence of a difference in means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97156f-e2e6-42a3-9b6f-4a254ff16558",
   "metadata": {},
   "source": [
    "## $\\chi^2$ test of association\n",
    "\n",
    "Tests relationships between categorical values.\n",
    "\n",
    "It does not make assumtions about distributions of the data\n",
    "\n",
    "To prepare for the test, we print the crosstab by training program and diet_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739fb0c6-430f-4224-8a2b-7bf83c3389e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "contingency_table = pd.crosstab(athletic_perf['Training_Program'], \n",
    "                               athletic_perf['Diet_Type'])\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea994e7c-b3e1-4467-ae7c-bd48e4e372f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f'chi2 statistic: {chi2_stat}, p_value: {p_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c56b1a-60ef-424b-ac11-c4533cebfc77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T16:41:59.393773Z",
     "iopub.status.busy": "2024-08-05T16:41:59.392831Z",
     "iopub.status.idle": "2024-08-05T16:41:59.408353Z",
     "shell.execute_reply": "2024-08-05T16:41:59.406577Z",
     "shell.execute_reply.started": "2024-08-05T16:41:59.393721Z"
    }
   },
   "source": [
    "> p_val > $\\alpha$ -> insufficient evidence of an association"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67698ea-059e-4af8-a576-5ddc8f3612fb",
   "metadata": {},
   "source": [
    "## Post-hoc analysis following ANOVA\n",
    "\n",
    "Post-hoc tests are pivotal when ANOVA reveals significant differences among groups. \n",
    "\n",
    "Allows to explore pairwise group differences\n",
    "\n",
    "2 Main post-hoc methods:\n",
    "\n",
    "- Tukey's HSD: robust for multiple comparisons. Besto for broader comparisons\n",
    "- Bonferroni correction: adjusts p-values to control for type I errors. Best for focused tests.\n",
    "\n",
    "Pandas pivot_table() method is very helpful when preparing the data for a post_hoc analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd5800-ae79-45bc-ad8a-3bee9d2d60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "therapy_outcomes = pd.read_csv('../data/therapy_outcomes.csv', index_col=0)\n",
    "therapy_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b69b6-bf02-45cb-a784-8319c669b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to view the mean anxiety reduction for each therapy\n",
    "pivot_table = therapy_outcomes.pivot_table(\n",
    "    values='Anxiety_Reduction', \n",
    "    index='Therapy_Type', \n",
    "    aggfunc=\"mean\")\n",
    "print(pivot_table)\n",
    "\n",
    "# Create groups to prepare the data for ANOVA\n",
    "therapy_types = ['CBT', 'DBT', 'ACT']\n",
    "groups = [therapy_outcomes[therapy_outcomes['Therapy_Type'] == therapy]['Anxiety_Reduction'] for therapy in therapy_types]\n",
    "\n",
    "# Conduct ANOVA\n",
    "f_stat, p_val = f_oneway(*groups)\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef5018-bc28-40b6-a913-333d852efc87",
   "metadata": {},
   "source": [
    "If ANOVA indicates significant differences, tukeys HSD test helps understanding exactly which values differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc021d-a344-4ab6-873b-e16f08bbb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777de753-61dc-4cf6-95a2-5f2f1ad52870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Tukey's HSD test\n",
    "tukey_results = pairwise_tukeyhsd(\n",
    "    therapy_outcomes['Anxiety_Reduction'], \n",
    "    therapy_outcomes['Therapy_Type'], \n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7de0d1-6008-40e1-bc0c-9bfc0a819e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "therapy_outcomes.Therapy_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4b8ed-fd79-4039-bd7e-f0f86d243fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.sandbox.stats.multicomp import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5af09-4513-41fb-b100-2f97690e75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = []\n",
    "\n",
    "therapy_pairs = [('CBT', 'DBT'), ('CBT', 'ACT'), ('DBT', 'ACT')]\n",
    "\n",
    "# Conduct t-tests and collect P-values\n",
    "for pair in therapy_pairs:\n",
    "    group1 = therapy_outcomes[therapy_outcomes['Therapy_Type'] == pair[0]]['Anxiety_Reduction']\n",
    "    group2 = therapy_outcomes[therapy_outcomes['Therapy_Type'] == pair[1]]['Anxiety_Reduction']\n",
    "    t_stat, p_val = ttest_ind(group1, group2)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "print(multipletests(p_values, alpha=0.05, method='bonferroni')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e2f5dd-e718-4e30-8d2c-59380f40d197",
   "metadata": {},
   "source": [
    "# P-values, alpha, and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f367c28-bf06-4eaf-bfef-3adddee3422e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T15:36:02.116497Z",
     "iopub.status.busy": "2024-08-12T15:36:02.116205Z",
     "iopub.status.idle": "2024-08-12T15:36:02.120064Z",
     "shell.execute_reply": "2024-08-12T15:36:02.119435Z",
     "shell.execute_reply.started": "2024-08-12T15:36:02.116477Z"
    }
   },
   "source": [
    "P-values are the probability of observing our data if the null hypothesis was true\n",
    "\n",
    "$\\alpha$ is the threshold at which we consider our results statistically significant. It indicates the probability of a type I error.\n",
    "\n",
    "If p-value $\\le$ $\\alpha$ -> reject null hypothesis in favor of alternative hypothesis \n",
    "\n",
    "Common values for $\\alpha$ are 0.1 (when higher tolerance for type I errors), 0.05 (most common), 0.01 (when the cost of a type I error is high), representing 10%, 5% and 1% risks of making a type I error.P-values are the probability of observing our data if the null hypothesis was true\n",
    "\n",
    "Choosing an $\\alpha$ depends on the context of the study and balances the tolerance for a type I error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e9dab-92b8-4e62-a3c7-eccb9ff1fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_durability=pd.read_csv('../data/toy_durability.csv')\n",
    "toy_durability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514bde1c-ccaf-415b-be3a-da4007665f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_durability.Toy_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e551fdd-1193-464a-9e61-10b12513bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean Durability_Score for each Toy_Type\n",
    "mean_durability = toy_durability.pivot_table(\n",
    "  values='Durability_Score', index='Toy_Type', aggfunc=np.mean)\n",
    "print(mean_durability)\n",
    "\n",
    "# Perform t-test\n",
    "educational_durability = toy_durability[toy_durability['Toy_Type'] == 'Educational']['Durability_Score']\n",
    "recreational_durability = toy_durability[toy_durability['Toy_Type'] == 'Recreational']['Durability_Score']\n",
    "t_stat, p_val = ttest_ind(educational_durability, recreational_durability)\n",
    "\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc448ca5-3b16-4d98-adbf-5aee05292af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of Durability_Score for each Toy_Type\n",
    "sns.displot(data=toy_durability, x=\"Durability_Score\", \n",
    "         hue=\"Toy_Type\", kind=\"kde\")\n",
    "plt.title('Durability Score Distribution by Toy Type')\n",
    "plt.xlabel('Durability Score')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de1644-ae75-4b65-a1bb-73c22db4e487",
   "metadata": {},
   "source": [
    "## Power Analysis: sample and effect size\n",
    "\n",
    "**Effect size** quantifies the difference between two groups. Cohen's d is a standard measure for effect size.\n",
    "\n",
    "**Power** the probability of correctly rejecting a false null hypothesis: (1-$\\beta$) where $\\beta$ is the probability of a type II error\n",
    "\n",
    "\n",
    "This high power tells us the likelihood that our test will detect a significant result, given our effect size and sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d432d-de43-4e7a-a62e-687156a02f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "power_analysis = TTestIndPower()\n",
    "power = power_analysis.solve_power(effect_size=1, nobs1=30, alpha=0.05)\n",
    "\n",
    "print(power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796443b3-6f12-4e3e-9001-358c498efae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TO REVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9826e4f-f6fa-4a39-ad1b-94c2d13ab363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a TTestIndPower object\n",
    "power_analysis = TTestIndPower()\n",
    "\n",
    "# Conduct a power analysis to determine the required sample size\n",
    "required_n = power_analysis.solve_power(\n",
    "    effect_size=0.5, \n",
    "    alpha=0.05, \n",
    "    power=0.9, \n",
    "    ratio=1)\n",
    "\n",
    "print(required_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b78761-f298-42d8-a0dd-90883c3c3170",
   "metadata": {},
   "source": [
    "We've determined that approximately 85 participants are required in each group to achieve a power of 0.9, assuming an Cohen's d effect size of 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b2900-e7b0-41fc-a72b-a1262d711493",
   "metadata": {},
   "source": [
    "## How to synthesize insights from complex experiments\n",
    "\n",
    "Graphs are a helpful tool for this purpose. They should be adapted depending on the audience.\n",
    "\n",
    "Technical audiences prefer data narratives including p-values, test statistics and significance levels in it. Charts can be advanced like heat maps or scatter plots with multiple colors and projection lines.\n",
    "\n",
    "A less technical audience prefers simplified data insights, basic graphs, content-centric presentations, with real world applications, and personalized reasons of why that matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b548db-5553-4428-b9e1-8b2bf67484a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_approval_yield = pd.read_csv('../data/loan_approval_yield.csv')\n",
    "customer_satisfaction = pd.read_csv('../data/customer_satisfaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1f8a6-537b-4bdb-81e3-b6a7758a7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Seaborn to create the bar graph\n",
    "sns.catplot(x=\"LoanAmount\", \n",
    "            y=\"ApprovalYield\", \n",
    "            hue=\"CreditScore\", \n",
    "            kind=\"bar\", \n",
    "            data=loan_approval_yield)\n",
    "plt.title(\"Loan Approval Yield by Amount and Credit Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197cfd9f-53ce-41f2-b260-02e57514f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two datasets\n",
    "merged_data = pd.merge(loan_approval_yield, \n",
    "                       customer_satisfaction, \n",
    "                       on='ApplicationID')\n",
    "\n",
    "# Use Seaborn to create the scatter plot\n",
    "sns.relplot(y=\"SatisfactionQuality\", \n",
    "            x=\"ApprovalYield\", \n",
    "            hue=\"InterestRate\", \n",
    "            kind=\"scatter\", \n",
    "            data=merged_data)\n",
    "plt.title(\"Satisfaction Quality by Approval Yield and Interest Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07802a17-7195-478a-b403-4e78b1c84307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-15T14:54:20.553087Z",
     "iopub.status.busy": "2024-08-15T14:54:20.552064Z",
     "iopub.status.idle": "2024-08-15T14:54:20.564550Z",
     "shell.execute_reply": "2024-08-15T14:54:20.563602Z",
     "shell.execute_reply.started": "2024-08-15T14:54:20.553010Z"
    }
   },
   "source": [
    "## Addresing complexities in experimental data\n",
    "\n",
    "Heteroscedasticity: changing variability of a variable across a range of another variable. The sns .residplot() method with the lowess=True can help us identifying this.\n",
    "\n",
    "When the residual plots deviates from expectations, its interesting to check the distribution of the variables used (distplot()). To address issues like skewness and heteroscedasticity, we can apply data transformations like Box-Cox\n",
    "\n",
    "\n",
    "\n",
    "Confounding variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1ae45-43f7-455e-8e6c-0b481b3c3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_preservation = pd.read_csv('../data/food_preservation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122b051-3bb0-47d7-ab3c-b6942731ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for heteroscedasticity with a residual plot\n",
    "sns.residplot(x='NutrientRetention', y='ShelfLife', \n",
    "         data=food_preservation, lowess=True)\n",
    "plt.title('Residual Plot of Shelf Life and Nutrient Retention')\n",
    "plt.xlabel('Nutrient Retention (%)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622df67f-14a0-4f7b-9aab-81c30423d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "# Visualize the original ShelfLife distribution\n",
    "sns.displot(food_preservation['ShelfLife'])\n",
    "plt.title('Original Shelf Life Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Create a Box-Cox transformation\n",
    "ShelfLifeTransformed, _ = boxcox(food_preservation['ShelfLife'])\n",
    "\n",
    "# Visualize the transformed ShelfLife distribution\n",
    "plt.clf()\n",
    "sns.displot(ShelfLifeTransformed)\n",
    "plt.title('Transformed Shelf Life Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891f6a5-c411-4e49-8bdb-05194c4e3e99",
   "metadata": {},
   "source": [
    "## Applying nonparametric tests in experimental analysis\n",
    "\n",
    "When:\n",
    "- Parametric test assumption are not met\n",
    "- Or data on ordinal scale or non normal\n",
    "- Need robustness against outliers or non linear data\n",
    "\n",
    "Visualizing nonparametric data can reval underlying patterns. Violin plots can help here. Boxen plots, an extension of box plots, display more information and gives more insight into the distribution shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac3cdb-da81-4835-8921-4cf445a1e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Filter to Freezing and Canning data\n",
    "condensed_food_data = food_preservation[food_preservation['PreservationMethod'].isin(['Freezing', 'Canning'])]\n",
    "\n",
    "# Create a violin plot for nutrient retention by preservation method\n",
    "sns.violinplot(data=condensed_food_data, \n",
    "               x=\"PreservationMethod\", \n",
    "               y=\"NutrientRetention\")\n",
    "plt.show()\n",
    "\n",
    "# Separate nutrient retention for Freezing and Canning methods\n",
    "freezing = food_preservation[food_preservation['PreservationMethod'] == 'Freezing']['NutrientRetention']\n",
    "canning = food_preservation[food_preservation['PreservationMethod'] == 'Canning']['NutrientRetention']\n",
    "\n",
    "# Perform Mann Whitney U test\n",
    "u_stat, p_val = mannwhitneyu(freezing, canning)\n",
    "\n",
    "# Print the p-value\n",
    "print(\"Mann Whitney U test p-value:\", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4825bfc3-4e83-467d-ad32-1935c62a6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "# Create a boxen plot for nutrient retention by preservation method\n",
    "sns.boxenplot(data=food_preservation, \n",
    "              x=\"PreservationMethod\", \n",
    "              y=\"NutrientRetention\")\n",
    "plt.show()\n",
    "\n",
    "# Separate nutrient retention for each preservation method\n",
    "freezing = food_preservation[food_preservation['PreservationMethod'] == 'Freezing']['NutrientRetention']\n",
    "canning = food_preservation[food_preservation['PreservationMethod'] == 'Canning']['NutrientRetention']\n",
    "drying = food_preservation[food_preservation['PreservationMethod'] == 'Drying']['NutrientRetention']\n",
    "\n",
    "# Perform Kruskal-Wallis test\n",
    "k_stat, k_pval = kruskal(freezing, canning, drying)\n",
    "print(\"Kruskal-Wallis test p-value:\", k_pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2378c6a-81e8-4a24-b525-64b523cc6baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
