{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd7217e6-2db0-4af2-b995-0f3c3ba99a1f",
   "metadata": {},
   "source": [
    "# Torchvision \n",
    "\n",
    "Pytorch module including pretrained models, datasets, preprocessing capabilities...\n",
    "\n",
    "# Image classification \n",
    "\n",
    "## Binary Classification \n",
    "\n",
    "## Multiclass Classification \n",
    "\n",
    "\n",
    "Data from (https://www.kaggle.com/code/tirendazacademy/cats-dogs-classification-with-pytorch/input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a192eb8e-35fa-4e40-9103-6e10c5e167d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:34:53.289964Z",
     "iopub.status.busy": "2025-01-08T11:34:53.288588Z",
     "iopub.status.idle": "2025-01-08T11:34:53.298458Z",
     "shell.execute_reply": "2025-01-08T11:34:53.297304Z",
     "shell.execute_reply.started": "2025-01-08T11:34:53.289891Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import datasets \n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f47d3f0b-9c2a-4e99-9f10-3c6911bee893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:34:53.796728Z",
     "iopub.status.busy": "2025-01-08T11:34:53.795559Z",
     "iopub.status.idle": "2025-01-08T11:34:53.807018Z",
     "shell.execute_reply": "2025-01-08T11:34:53.804770Z",
     "shell.execute_reply.started": "2025-01-08T11:34:53.796661Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    pass\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dfeacb3-d689-4b89-aeb0-6a23cbe323e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:34:54.091346Z",
     "iopub.status.busy": "2025-01-08T11:34:54.090474Z",
     "iopub.status.idle": "2025-01-08T11:34:54.624055Z",
     "shell.execute_reply": "2025-01-08T11:34:54.622774Z",
     "shell.execute_reply.started": "2025-01-08T11:34:54.091279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.6)\n",
      "Path to dataset files: /Users/el_fer/.cache/kagglehub/datasets/tongpython/cat-and-dog/versions/1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"tongpython/cat-and-dog\")\n",
    "\n",
    "print(\"Path to dataset files:\", dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83ccd207-1fc8-4bac-83a9-b84df2f3554e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:34:54.627948Z",
     "iopub.status.busy": "2025-01-08T11:34:54.627299Z",
     "iopub.status.idle": "2025-01-08T11:34:54.692074Z",
     "shell.execute_reply": "2025-01-08T11:34:54.691644Z",
     "shell.execute_reply.started": "2025-01-08T11:34:54.627902Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_path = dataset_path + '/training_set/training_set'\n",
    "train_dataset = datasets.ImageFolder(root=train_dataset_path, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce53e503-c5d6-4994-bebe-0089bc4cd5b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:34:54.900502Z",
     "iopub.status.busy": "2025-01-08T11:34:54.899656Z",
     "iopub.status.idle": "2025-01-08T11:34:54.916834Z",
     "shell.execute_reply": "2025-01-08T11:34:54.916029Z",
     "shell.execute_reply.started": "2025-01-08T11:34:54.900436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats', 'dogs']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56302d-d740-4e8a-bf5a-449b6c5db94d",
   "metadata": {},
   "source": [
    "# Pre-Trained models \n",
    "\n",
    "Training models from scratch is a long process that requires a lot of data.\n",
    "\n",
    "There are already trained models (pre-trained models) that can be either directly reusable on our task if its similar to the original or that can be adjusted to the new task (transfer learning)\n",
    "\n",
    "## Saving a model\n",
    "\n",
    "In pytorch you can save the model weigths using \n",
    "\n",
    "`torch.save(model.state_dict(), \"BinaryCNN.pth\")`\n",
    "\n",
    "You can use the extensions `.pt` or `.pth` \n",
    "\n",
    "## Loading models \n",
    "\n",
    "Loading a pretrained model happens in two steps: first, instantiate the model and then, load the weigths: \n",
    "\n",
    "`new_model = BinaryCNN()` \n",
    "\n",
    "`new_model.load_state_dict(torch.load(\"BinaryCNN.pth\"))` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba36b345-cbdb-492a-9d53-739e7170c7ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:35:02.289000Z",
     "iopub.status.busy": "2025-01-08T11:35:02.287789Z",
     "iopub.status.idle": "2025-01-08T11:35:02.522865Z",
     "shell.execute_reply": "2025-01-08T11:35:02.522330Z",
     "shell.execute_reply.started": "2025-01-08T11:35:02.288929Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models import (resnet18, ResNet18_Weights)\n",
    "\n",
    "weights = ResNet18_Weights.DEFAULT \n",
    "model = resnet18(weights=weights)\n",
    "# the transforms allow us to process the data in order to feed the model with what it expects\n",
    "transforms = weights.transforms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47ca51c5-178f-44a8-bda6-7dbd9aa8b858",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:35:04.128164Z",
     "iopub.status.busy": "2025-01-08T11:35:04.125944Z",
     "iopub.status.idle": "2025-01-08T11:35:04.156178Z",
     "shell.execute_reply": "2025-01-08T11:35:04.155666Z",
     "shell.execute_reply.started": "2025-01-08T11:35:04.128078Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "\n",
    "image = Image.open(\"cat2.jpg\")\n",
    "image_tensor = transforms(image)\n",
    "image_reshaped = image_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3af61dbc-0617-4683-bd3b-0f0c36b57309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:35:05.447769Z",
     "iopub.status.busy": "2025-01-08T11:35:05.446935Z",
     "iopub.status.idle": "2025-01-08T11:35:05.545501Z",
     "shell.execute_reply": "2025-01-08T11:35:05.544994Z",
     "shell.execute_reply.started": "2025-01-08T11:35:05.447728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese cat\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "\n",
    "with torch.no_grad(): \n",
    "    pred = model(image_reshaped).squeeze(0) \n",
    "\n",
    "pred_cls = pred.softmax(0) \n",
    "cls_id = pred_cls.argmax().item() \n",
    "\n",
    "cls_name = weights.meta[\"categories\"][cls_id]  \n",
    "\n",
    "print(cls_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afddc94-6c1c-4309-a941-9226fab8cbf8",
   "metadata": {},
   "source": [
    "# Object Recognition \n",
    "\n",
    "Identification of objects in images. Its often performed using bounding boxes. \n",
    "\n",
    "## Bounding boxes \n",
    "\n",
    "Rectangular box describing the object spatial location within the image \n",
    "\n",
    "The ground truth bounding box contains the object precise location \n",
    "\n",
    "Bounding boxes are tipically represented using the top left and bottom right coordinates.\n",
    "\n",
    "`Bounding box = (x1, y1, x2, y2)`\n",
    "\n",
    "Images are composed of pixels arranged into a 2d matrix. The origin is the top left corner, with coordinates (0,0)\n",
    "\n",
    "To be able to process images in pytorch we must convert pixels into tensors. \n",
    "\n",
    "The `ToTensor()` method converts images to tensors (torch.float type and scaled between [0.0, 1-0])\n",
    "\n",
    "The `PILToTensor()` method converts images to tensors (torch.unint8 type and scaled between [0, 255])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "813e301d-056e-49c8-bca8-565abca35bb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T13:16:49.180918Z",
     "iopub.status.busy": "2025-01-08T13:16:49.180124Z",
     "iopub.status.idle": "2025-01-08T13:16:49.227947Z",
     "shell.execute_reply": "2025-01-08T13:16:49.227332Z",
     "shell.execute_reply.started": "2025-01-08T13:16:49.180881Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224), \n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "image = Image.open(\"cat2.jpg\")\n",
    "\n",
    "image_tensor = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "63bcc1bd-bff0-4f58-b1e7-646e9c381927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T13:16:49.989772Z",
     "iopub.status.busy": "2025-01-08T13:16:49.988962Z",
     "iopub.status.idle": "2025-01-08T13:16:50.000478Z",
     "shell.execute_reply": "2025-01-08T13:16:49.999001Z",
     "shell.execute_reply.started": "2025-01-08T13:16:49.989716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7643f0fe-7785-49a6-8b47-472efea7407f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T13:18:43.527217Z",
     "iopub.status.busy": "2025-01-08T13:18:43.525983Z",
     "iopub.status.idle": "2025-01-08T13:18:43.536953Z",
     "shell.execute_reply": "2025-01-08T13:18:43.535274Z",
     "shell.execute_reply.started": "2025-01-08T13:18:43.527145Z"
    }
   },
   "outputs": [],
   "source": [
    "image_tensor = image_tensor.permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc27fb-de40-4fee-9916-5a6772a90aa4",
   "metadata": {},
   "source": [
    "## Drawing bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6bccc004-8101-4c3f-b826-941b640557ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:37:47.066774Z",
     "iopub.status.busy": "2025-01-08T15:37:47.065339Z",
     "iopub.status.idle": "2025-01-08T15:37:47.119414Z",
     "shell.execute_reply": "2025-01-08T15:37:47.118590Z",
     "shell.execute_reply.started": "2025-01-08T15:37:47.066704Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tx/44c409ls4bvf__rjhb2qjmf40000gn/T/ipykernel_72139/154154066.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bbox_tensor = torch.tensor(bbox).unsqueeze(0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only grayscale and RGB images are supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m bbox_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(bbox)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Implement draw_bounding_boxes\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m img_bbox \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Tranform tensors to image\u001b[39;00m\n\u001b[1;32m     11\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToPILImage()\n\u001b[1;32m     13\u001b[0m ])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BaseML/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BaseML/lib/python3.11/site-packages/torchvision/utils.py:201\u001b[0m, in \u001b[0;36mdraw_bounding_boxes\u001b[0;34m(image, boxes, labels, colors, fill, width, font, font_size)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass individual images, not batches\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m}:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly grayscale and RGB images are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (boxes[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m boxes[:, \u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m (boxes[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m boxes[:, \u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoxes need to be in (xmin, ymin, xmax, ymax) format. Use torchvision.ops.box_convert to convert them\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Only grayscale and RGB images are supported"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import draw_bounding_boxes \n",
    "\n",
    "# bbox = torch.tensor([x_min, y_min, x_max, y_max])\n",
    "bbox = torch.tensor([2, 2, 40, 40])\n",
    "bbox_tensor = torch.tensor(bbox).unsqueeze(0)\n",
    "\n",
    "# Implement draw_bounding_boxes\n",
    "img_bbox = draw_bounding_boxes(image_tensor, bbox_tensor, width=3, colors=\"red\")\n",
    "\n",
    "# Tranform tensors to image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "plt.imshow(transform(img_bbox))\n",
    "plt.show()\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a4950d-25c2-4204-af85-6e538e3c82dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8effe94c-05a8-4b2c-847c-fa4f2583556f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
