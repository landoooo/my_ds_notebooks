{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e54fbd-1d04-4b02-a59b-83638c95ad9e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "**Sentiment analysis** is the process of understanding the opinion of an author about a subject. \n",
    "\n",
    "## Key Elements \n",
    "\n",
    "- **Opinion/Emotion**\n",
    "    - Opinion (or polarity) can be positive, neutral or negative\n",
    "    - Emotion can be qualitative\n",
    "\n",
    "- **Subject**\n",
    "\n",
    "- **Opinion Holder**\n",
    "\n",
    "Sentiment analysis is important in social media monitoring, brand monitoring, customer service, product analytics, market research..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb4995-1458-47da-8046-fa45a6498e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "data = pd.read_csv('../data/sentiment_analysis/IMDB_sample.csv')\n",
    "\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e6cb1-9ab6-4039-b26a-5b62a9bae0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95f1f0-e20a-458a-98cd-de9dea21ece1",
   "metadata": {},
   "source": [
    "## Levels of granularity\n",
    "\n",
    "- Document\n",
    "- Sentence\n",
    "- Aspect\n",
    "\n",
    "## Type of sentiment analysis algorithms \n",
    "\n",
    "### Rule/lexicon-based \n",
    "\n",
    "nice:+2, good:+1, terrible:-3... \n",
    "\n",
    "### Automatic / Machine Learning \n",
    "\n",
    "### Which one to choose? \n",
    "\n",
    "The automated/ML approach depends on historical labelled data, takes longer and can be quite powerful, while the rule/lexicon depends on manually crafted valence scores, can be fast but it different words might have different values depending on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78b949-0a03-46fc-b2f0-fafa58495ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Today was a good day.\"\n",
    "\n",
    "from textblob import TextBlob \n",
    "\n",
    "my_valence = TextBlob(text) \n",
    "print(my_valence.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df22c76-4e4e-49fe-aa11-990213b4d117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T20:05:26.128588Z",
     "iopub.status.busy": "2024-11-10T20:05:26.127217Z",
     "iopub.status.idle": "2024-11-10T20:05:26.136168Z",
     "shell.execute_reply": "2024-11-10T20:05:26.134890Z",
     "shell.execute_reply.started": "2024-11-10T20:05:26.128543Z"
    }
   },
   "source": [
    "## Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b0141-3bdd-47bc-89ab-0fab6df96044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud \n",
    "\n",
    "cloud_two_cities = WordCloud().generate('En un pais multicolor nacion una abeja bajo el sol. La abeja se llamaba Maya y era azteca, que no maya. La abeja del pais guay')\n",
    "\n",
    "plt.imshow(cloud_two_cities, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23264f2a-9df5-433b-9d3d-f187130c61ac",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)\n",
    "\n",
    "Transforms the text into a sort of numeric form.\n",
    "\n",
    "BoW describes the occurence of words within a document or a collection of documents. \n",
    "\n",
    "It builds a vocabulary of the words and a measure of their presence.\n",
    "\n",
    "The inconvenients of BoW are linked to the fact that word order and grammar rules are lost once we build our BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccedbfa-3e00-4f99-a230-5a790607d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = pd.read_csv('../data/sentiment_analysis/amazon_reviews_sample.csv')\n",
    "revs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5524de9-bb67-45bf-8cf9-6c000f7a9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "cv.fit(data.review)\n",
    "X=cv.transform(data.review) \n",
    "X_df = pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46301a3-71e5-4a8f-9127-240b78cc5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f678e-ee5d-4070-998e-c43ab9a0f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "sys.getsizeof(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d073c67-36f6-4438-bb68-bc66855e94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1770a0-774a-4380-880c-21ce9b029529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T09:16:30.968446Z",
     "iopub.status.busy": "2024-11-11T09:16:30.967665Z",
     "iopub.status.idle": "2024-11-11T09:16:30.975880Z",
     "shell.execute_reply": "2024-11-11T09:16:30.974321Z",
     "shell.execute_reply.started": "2024-11-11T09:16:30.968408Z"
    }
   },
   "source": [
    "# N-grams\n",
    "\n",
    "Negations are somehow neglected by BoW.\n",
    "\n",
    "- Unigrams: single tokens \n",
    "- Bigrams: tuples\n",
    "- Trigrams:\n",
    "- N-grams\n",
    "\n",
    "\n",
    "`cv = CountVectorizer(ngram_range=(min_n, max_n))`\n",
    "\n",
    "`ngram_range(1,2)` for instance, uses unigrams and bigrams\n",
    "\n",
    "The longer the n:\n",
    "- The more precise the ML model\n",
    "- More features\n",
    "- Higher risk of overfitting\n",
    "\n",
    "Look for the best n for the problem at hand. \n",
    "\n",
    "- the `max_features` parameter helps containing the number of features, selecting the most frequent words only.\n",
    "- `max_df` ignore terms with a frequency above its value. Can be an integer, or a proportion. Same for `min_df`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b618b-62dd-4f42-a61e-f8d5aff5c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Build the vectorizer, specify max features and fit\n",
    "vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)\n",
    "vect.fit(revs.review)\n",
    "\n",
    "# Transform the review\n",
    "X_review = vect.transform(revs.review)\n",
    "\n",
    "# Create a DataFrame from the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names_out())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0b14a-85f0-4fe1-921f-355d98ae14f5",
   "metadata": {},
   "source": [
    "# Build features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d6030-2619-45c8-831c-ae72b40ee859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize \n",
    "\n",
    "anna_k = 'Happy families are all alike, every unhappy family is unhappy in its own way.'\n",
    "\n",
    "word_tokenize(anna_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383460c6-c278-4629-96d5-8bc8eeef776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = [word_tokenize(review) for review in revs.review] \n",
    "word_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aba8b0-1a26-41cc-93f0-8074742798c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_tokens = [] \n",
    "\n",
    "for i in range(len(word_tokens)): \n",
    "    len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "revs['len_tokens'] = len_tokens\n",
    "revs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef360f1-5713-41f0-b663-29a348df54ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "\n",
    "spanish = 'Tu crees que el tema este será capaz de adivinar en qué idioma está escrita esta frase?'\n",
    "\n",
    "detect_langs(spanish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdf271-4cf6-4e21-b172-17cb6ce292f3",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "Frequent words that occur very often and that dont add much value.\n",
    "\n",
    "Depending on the context we would like to add other words that we know will be very frequent too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de742782-acc6-4c22-95b4-61a29a2f7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "my_stopwords =set(STOPWORDS)\n",
    "# When analysing movie reviews, the following words can be considered stop words too\n",
    "my_stopwords.update([\"movie\", \"movies\", \"film\", \"films\", \"watch\", \"br\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcfd919-af4f-4ef6-887b-f8cf4b0ba68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cloud = WordCloud(background_color='white', stopwords=my_stopwords).generate(data.review[0])\n",
    "plt.imshow(my_cloud, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52f6fd-959b-48b0-90ab-613119968c0b",
   "metadata": {},
   "source": [
    "## Stopwords with BoW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e2459-6efa-4bde-9046-217848861034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "my_stopwords = ENGLISH_STOP_WORDS.union([\"movie\", \"movies\", \"film\", \"films\", \"watch\", \"br\"])\n",
    "\n",
    "vect=CountVectorizer(stop_words=list(my_stopwords))\n",
    "vect.fit(data.review)\n",
    "X=vect.transform(data.review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7720c-a6e8-4619-b1c6-341d01cbe0c4",
   "metadata": {},
   "source": [
    "# Capturing tokens using patterns \n",
    "\n",
    "Sometimes there are tokens we would like to ignore, like mails, digits..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e5c0a-663c-4093-9244-24ee3a0a0ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = '123'\n",
    "print('isalnum', my_string.isalnum())\n",
    "print('isalpha', my_string.isalpha())\n",
    "print('isdecimal', my_string.isdecimal())\n",
    "print('isdigit', my_string.isdigit())\n",
    "print('isnumeric', my_string.isnumeric())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d57463-af98-4d35-bf3e-58754513c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "my_string = '#cocotero'\n",
    "\n",
    "x=re.search('#[A-Za-z]+', my_string)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a46477-19f8-4710-bc70-c969c156792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../data/sentiment_analysis/Tweets.csv')\n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(tweets.text)\n",
    "vect.transform(tweets.text)\n",
    "print('Length of vectorizer: ', len(vect.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f3351-5a1d-455a-bb54-7c9fe715f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the first vectorizer\n",
    "vect1 = CountVectorizer().fit(tweets.text)\n",
    "vect1.transform(tweets.text)\n",
    "\n",
    "# Build the second vectorizer\n",
    "vect2 = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]').fit(tweets.text)\n",
    "vect2.transform(tweets.text)\n",
    "\n",
    "# Print out the length of each vectorizer\n",
    "print('Length of vectorizer 1: ', len(vect1.get_feature_names_out()))\n",
    "print('Length of vectorizer 2: ', len(vect2.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c1555-584d-422d-9759-e2f32a983991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenizing package\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize the text column\n",
    "word_tokens = [word_tokenize(review) for review in tweets.text]\n",
    "print('Original tokens: ', word_tokens[0])\n",
    "\n",
    "# Filter out non-letter characters\n",
    "cleaned_tokens = [[word for word in item if word.isalpha()] for item in word_tokens]\n",
    "print('Cleaned tokens: ', cleaned_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6daa40-3136-4749-8d2d-70018724a9d5",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "Stemming is the process of reducing the words of a text to their roots. Faster\n",
    "Lemmatization is similar to stemming but instead of finding roots, it reduces each word to actual valid words. Slower.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85645a8-1b46-47a2-97e2-ebefdbc040be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "porter = PorterStemmer()\n",
    "porter.stem('wonderful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c24e57-b94a-4963-bdcd-55d5fda00edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer \n",
    "\n",
    "SpanishStemmer = SnowballStemmer('spanish')\n",
    "SpanishStemmer.stem('cuadranguloso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401e55a-3148-45e7-88c2-b252cc4599f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Stem doesnt apply to sentences'\n",
    "porter.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55315eff-92ba-439b-a176-beb96b46c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize \n",
    "\n",
    "tokens=word_tokenize(sentence)\n",
    "stemmed_tokens = [porter.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ff302-06a5-4571-b44f-f0d4b68b0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9f905-d32a-486c-8c95-f3645f24d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# pos is part of speech\n",
    "lemmatizer.lemmatize('wonderful', pos='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48945251-6062-4017-8a6a-6028c65e2382",
   "metadata": {},
   "source": [
    "# TFIDF\n",
    "\n",
    "The tfidf score is defined as term frequency * inverse document frequency.\n",
    "\n",
    "BoW does not account for the lenght of a document. TFIDF does. \n",
    "\n",
    "TFIDF takes into account words that are common in all the documents\n",
    "\n",
    "TFIDF due to its nature, doesnt need to take care of stopwords explicitly as other methods do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f14121-5a69-4d91-8adb-c318301b2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "vect = TfidfVectorizer(max_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a995d1-907b-4765-96e5-dd8cd54b91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(tweets.text)\n",
    "X=vect.transform(tweets.text)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6e7b7-eca5-411a-9755-b0229df1a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X.toarray(), columns=vect.get_feature_names_out())\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5c5ba-a04a-42ca-a0bf-df0dc5ddf888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required vectorizer package and stop words list\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the vectorizer and specify the arguments\n",
    "my_pattern = r'\\b[^\\d\\W][^\\d\\W]+\\b'\n",
    "vect = TfidfVectorizer(ngram_range=(1, 2), max_features=100, token_pattern=my_pattern, stop_words=list(ENGLISH_STOP_WORDS)).fit(tweets.text)\n",
    "\n",
    "# Transform the vectorizer\n",
    "X_txt = vect.transform(tweets.text)\n",
    "\n",
    "# Transform to a data frame and specify the column names\n",
    "X=pd.DataFrame(X_txt.toarray(), columns=vect.get_feature_names_out())\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287be877-e083-4f64-9808-2c2abab7b813",
   "metadata": {},
   "source": [
    "# Predicting Sentiment \n",
    "\n",
    "Classification problem with 2 classes (positive or negative) or 3 (positive, neutral and negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc16d5-01f9-4911-b2c0-f34a110e0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vect=CountVectorizer(max_features=100)\n",
    "\n",
    "movies = pd.read_csv('../data/sentiment_analysis/IMDB_sample.csv')\n",
    "\n",
    "X = vect.fit_transform(movies.review)\n",
    "\n",
    "# Define the vector of targets and matrix of features\n",
    "y = movies.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Build a logistic regression model and calculate the accuracy\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of logistic regression: ', log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb3656-22ed-45b4-ab37-d6dada0a6928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
