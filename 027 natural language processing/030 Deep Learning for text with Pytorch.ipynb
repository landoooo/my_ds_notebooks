{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c24842-8884-40b8-879b-ceaed5b711c3",
   "metadata": {},
   "source": [
    "# Text Processing Pipeline \n",
    "\n",
    "Raw data > preprocessing > encoding > dataset and dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045996a-3b72-4772-a3da-d7b3d0a1cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265dd9c-323a-483d-8a44-9b655195c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6036c-1246-45fb-8212-aa45e2a57cee",
   "metadata": {},
   "source": [
    "# Preprocessing \n",
    "\n",
    "Preprocessing reduces the amount of features, providing cleaner and more representative datasets.\n",
    "\n",
    "## Tokenization \n",
    "\n",
    "Tokens can be words, parts of words or punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7a29d-288b-4a1c-99e8-8f9d2ded2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english') \n",
    "tokens = tokenizer('I am reading a book now. I love to read books!') \n",
    "ic(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f05b2c-db69-4ea6-b606-08d0d30c2bd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T08:48:04.413254Z",
     "iopub.status.busy": "2024-12-10T08:48:04.412732Z",
     "iopub.status.idle": "2024-12-10T08:48:04.431095Z",
     "shell.execute_reply": "2024-12-10T08:48:04.429650Z",
     "shell.execute_reply.started": "2024-12-10T08:48:04.413221Z"
    }
   },
   "source": [
    "## Stop word removal \n",
    "\n",
    "Eliminate common words that do not contribute to the meaning.\n",
    "\n",
    "Stop words: 'a', 'the', 'or' and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4173cc20-305c-4647-910a-9c694e170db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "ic(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc50f2-61b4-4282-97a8-d00303533ed7",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Reduce words to their base form \n",
    "\n",
    "For example: *running*, *runs* and *ran* becomes **run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014882db-b55e-4c02-b579-94d883162cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "print(stemmed_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d9ecd6-d45b-419a-9b81-76c1f4d3ee7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T08:56:35.194341Z",
     "iopub.status.busy": "2024-12-10T08:56:35.193832Z",
     "iopub.status.idle": "2024-12-10T08:56:35.201271Z",
     "shell.execute_reply": "2024-12-10T08:56:35.199279Z",
     "shell.execute_reply.started": "2024-12-10T08:56:35.194309Z"
    }
   },
   "source": [
    "## Rare word removal \n",
    "\n",
    "Removing infrequent words that dont add value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c37de1-7d83-4c64-bb9b-21caaa571243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist \n",
    "\n",
    "freq_dist = FreqDist(stemmed_tokens)\n",
    "threshold = 1\n",
    "\n",
    "common_tokens = [token for token in stemmed_tokens if freq_dist[token] > threshold]\n",
    "\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9118e-ec6f-48e2-8f39-790b6c8ba43a",
   "metadata": {},
   "source": [
    "# Encoding Text Data\n",
    "\n",
    "Converts text into machine-readable numbers \n",
    "\n",
    "Enables analysis and modeling \n",
    "\n",
    "## One Hot Encoding \n",
    "\n",
    "Mapping each word to a distinct vector \n",
    "\n",
    "Binary Vector: \n",
    "- 1 for the presence of a word\n",
    "- 0 for the absence of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc7bcb-469b-40be-9363-acc96c3a2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "vocab = ['cat', 'dog', 'rabbit']\n",
    "\n",
    "vocab_size = len(vocab) \n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "ic(one_hot_vectors)\n",
    "one_hot_dict = {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "ic(one_hot_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056cc12-a21d-4f79-8057-71dd34975541",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "\n",
    "Treats the text as an unordered collection of words and takes into account the frequency of each word, not the order in which it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154fe4d-859d-4924-bf33-805764cba4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer() \n",
    "\n",
    "corpus= ['Hey, look at me! I am a document', \n",
    "         'So you think you are special, first document? I am a document too', \n",
    "         'Guys, please. We are all documents, ok? Calm down.'\n",
    "        ]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c367c104-bfe0-4d7a-8fa6-f3f65076b3b6",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency - Inverse Document Frequency) \n",
    "\n",
    "Scores the importance of words in a document, taking into account the presence of words in other documents: specific words on a document are scored higher and common words present in every document are sored lower. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f890f-f301-49a9-b193-4bee81a67ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "vectorizer = TfidfVectorizer() \n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray()) \n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b823f28-201d-4879-b197-8c542325e3a5",
   "metadata": {},
   "source": [
    "## Preprocessing in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3047aa82-a94b-4973-baf0-3c64e2da6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset): \n",
    "    def __init__(self, data): \n",
    "        self.data = data \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a8e87-d6e9-4909-9214-345736acfeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences): \n",
    "    processed_sentences = [] \n",
    "    for sentence in sentences: \n",
    "        sentence = sentence.lower() \n",
    "        tokens = tokenizer(sentence) \n",
    "        tokens = [token for token in tokens if token not in stop_words] \n",
    "        tokens = [stemmer.stem(token) for token in tokens] \n",
    "        freq_dist = FreqDist(tokens) \n",
    "        threshold = 0\n",
    "        tokens = [token for token in tokens if freq_dist[token]>threshold] \n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade99771-db10-41b3-a9ed-1236a2455e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_sentences(['This is the first text data. And here is another one. What do you think about being just data. This is not the first time i think about it. Just being data.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9997834-9214-444f-8bf0-47cd23429404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences): \n",
    "    vectorizer = CountVectorizer() \n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    encoded_sentences = X.toarray()\n",
    "    return encoded_sentences, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb0ff8-15d2-47a7-b0f2-9e8210c1c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sentences(data): \n",
    "    sentences = re.findall(r'[A-Z][^.!?]*[.!?]', data) \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba100889-55da-4445-bfb7-a47d83c335a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_pipeline(text): \n",
    "    tokens = preprocess_sentences(text) \n",
    "    encoded_sentences, vectorizer = encode_sentences(tokens) \n",
    "    print(encoded_sentences)\n",
    "    dataset = TextDataset(encoded_sentences) \n",
    "    dataloader = DataLoader(dataset, batch_size=2)\n",
    "    return dataloader, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e49f5-3b88-4f45-924d-181c7d173241",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = 'This is the first text data. And here is another one.'\n",
    "\n",
    "sentences = extract_sentences(text_data)\n",
    "\n",
    "dataloader, vectorizer = [text_preprocessing_pipeline(sentences) for text in sentences]\n",
    "\n",
    "for data in dataloader: \n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe983159-35c5-4511-b627-8424c283b2be",
   "metadata": {},
   "source": [
    "## Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d79ff-b84e-4f4f-8355-97b7142bc024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "from torch import nn \n",
    "\n",
    "words = ['The', 'cat', 'sat', 'on', 'the', 'mat'] \n",
    "word_to_idx = {word:i for i, word in enumerate(words)}\n",
    "\n",
    "inputs = torch.LongTensor([word_to_idx[w] for w in words])\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=len(words), embedding_dim=10)\n",
    "output = embedding(inputs)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7236a51-bf1f-4390-be07-378ba715f550",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e5df4-cd5d-49af-90b4-1f69b272b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "class TextDataset(Dataset): \n",
    "    def __init__(self, text): \n",
    "        self.text = text \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.text)  \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        return self.text[idx]\n",
    "\n",
    "class LSTMModel(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, output_size): \n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x): \n",
    "        _, (hidden, _) = self.lstm(x) \n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "class GRUModel(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, output_size): \n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x): \n",
    "        _, (hidden, _) = self.gru(x) \n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec33624-32bd-46d8-aaa9-5885c8f47168",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d7305-0952-473a-bd12-a5280b2ca7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "data = 'Hello, how are you?' \n",
    "chars = list(set(data)) \n",
    "ic(chars) \n",
    "\n",
    "char_to_ix = {char:i for i, char in enumerate(chars)}\n",
    "ix_to_char =  {i:char for i, char in enumerate(chars)}\n",
    "\n",
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "      h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "      out, _ = self.rnn(x, h0)  \n",
    "      out = self.fc(out[:, -1, :])  \n",
    "      return out\n",
    "\n",
    "model = RNNmodel(1, 16, 1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150c38d-aa20-4d00-b340-375a2795e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [char_to_ix[ch] for ch in data[:-1]]\n",
    "ic(inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[1:]]\n",
    "ic(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50a051-e91f-44a1-8d8a-1feb050e4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(inputs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f7eb9-eb1a-4902-b621-aa7ee5155130",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = nn.functional.one_hot(inputs, num_classes=len(chars)).float()\n",
    "targets = torch.tensor(targets, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0765273-cd09-490d-b737-3a29ce0eaa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Instantiate the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/100, Loss: {loss.item()}')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_input = char_to_ix['r']\n",
    "test_input = nn.functional.one_hot(torch.tensor(test_input).view(-1, 1), num_classes=len(chars)).float()\n",
    "predicted_output = model(test_input)\n",
    "predicted_char_ix = torch.argmax(predicted_output, 1).item()\n",
    "print(f\"Test Input: 'r', Predicted Output: '{ix_to_char[predicted_char_ix]}'\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e050e37-5c0a-447a-9b25-c57b8cad5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d15aa3-9281-4c62-9ee9-fcc12836fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.zeros(18, 16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc6b26f-0f72-444c-9b4d-2f0369e13c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2fb39-7084-4964-b7eb-0727f842bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2') \n",
    "seed_text = 'Once upon a time' \n",
    "\n",
    "input_ids = tokenizer.encode(seed_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef5400-08c5-4d2f-b1ff-3a18831d846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_length=40, temperature=0.7, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True) \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4fcc36-7368-4457-b62c-18c4fba9a71c",
   "metadata": {},
   "source": [
    "# Evaluating Text Generation \n",
    "\n",
    "Text generation tasks create human like text. \n",
    "\n",
    "Standard accuracy metrics such as accuracy, F1 fall short for these tasks \n",
    "\n",
    "We need metrics that evaluate the quality of generated text \n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) and ROUGE ()\n",
    "\n",
    "## BLEU\n",
    "\n",
    "Compares the generated text and the reference text.\n",
    "\n",
    "Checks for the occurence of n grams (sequences of n words)\n",
    "\n",
    "The more matches the best score. The perfect score is 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabbf95-303a-43c6-8329-c0d4938d1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import BLEUScore\n",
    "\n",
    "generated_text = ['The cat is on the mat']\n",
    "real_text = [['there is a cat on the mat', 'a cat is on the mat']]\n",
    "\n",
    "bleu = BLEUScore() \n",
    "bleu_metric = bleu(generated_text, real_text)\n",
    "ic(bleu_metric.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9edfef-6aa8-4db7-b83d-c9765638be0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T15:19:10.339522Z",
     "iopub.status.busy": "2024-12-12T15:19:10.338937Z",
     "iopub.status.idle": "2024-12-12T15:19:10.348906Z",
     "shell.execute_reply": "2024-12-12T15:19:10.344811Z",
     "shell.execute_reply.started": "2024-12-12T15:19:10.339489Z"
    }
   },
   "source": [
    "## ROUGE(Recall-Oriented Understudy for Gisting Evaluation) \n",
    "\n",
    "Compares a generated text to a reference text in two ways\n",
    "\n",
    "ROUGE-N: considers overlapping n-grams in both texts \n",
    "\n",
    "ROUGE-L: looks at the longest common subsequence LCS between the texts\n",
    "\n",
    "ROUGE Metrics: \n",
    "- F-measure : Harmonic mean of precision and recall\n",
    "- Precision: Matches of n-grams in generated text within the reference text\n",
    "- Recall: Matches of n-grams in reference text within the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f716b9-4cc5-470b-961f-108560d11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import ROUGEScore \n",
    "\n",
    "generated_text = 'Hello, how are you doing?' \n",
    "real_text = 'Hello, how are you?' \n",
    "\n",
    "rouge = ROUGEScore() \n",
    "\n",
    "rouge_score = rouge([generated_text],[[real_text]])\n",
    "ic(rouge_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b83410-56d1-41c5-9a73-22ad3295c7d1",
   "metadata": {},
   "source": [
    "## Considerations and limitations \n",
    "\n",
    "Evaluate word presence not semantic understanding \n",
    "\n",
    "Sensitive to the lenght of the generated text \n",
    "\n",
    "Quality of reference text is crucial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48cc18e-0cd5-472f-8270-546cd3712d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning for text classification \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb235bdd-5551-4618-b51a-6f729f6c1438",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I love this!\", \n",
    "    \"This is terrible.\", \n",
    "    \"Amazing experience!\", \n",
    "    \"Not my cup of tea.\" \n",
    "]\n",
    "\n",
    "labels = [1,0,1,0]\n",
    "\n",
    "import torch \n",
    "from transformers import BertTokenizer, BertForSequenceClassification \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=32) \n",
    "inputs['labels'] = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb45bc-a7f0-4b80-90d1-98258943b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "model.train()\n",
    "for epoch in range(1): \n",
    "    outputs = model(**inputs) \n",
    "    loss = outputs.loss \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    optimizer.zero_grad()\n",
    "    print(f'Epoch:{epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c26a9-b806-491e-adaf-d2fd8aa5772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I had an awesome day!' \n",
    "\n",
    "input_eval = tokenizer(text, return_tensors = 'pt', truncation = True, padding=True, max_length=128) \n",
    "\n",
    "outputs_eval = model(**input_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c8b95-7a88-4af4-af42-ffce87ad9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84bbfce-243e-40af-9b5a-9b753b30c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804b6da-5265-49c2-829a-040c7607db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =torch.nn.functional.softmax(outputs_eval.logits, dim=-1)\n",
    "predicted_label = 'positive' if torch.argmax(predictions) > 0  else 'negative'\n",
    "print(f'text: {text} \\nSentiment: {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449b97e-80d2-4835-8262-ebf84c7699db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
