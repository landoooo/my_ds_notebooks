{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1309988-b429-4fb0-8c4c-193582dbec93",
   "metadata": {},
   "source": [
    "![mobydick](mobydick.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e416c-70e7-478a-a3c8-e54f3fdb4a7f",
   "metadata": {},
   "source": [
    "In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n",
    "\n",
    "The Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d01e132-fa56-48d5-9c7f-c643ed398c9c",
   "metadata": {},
   "source": [
    "What are the most frequent words in Herman Melville's novel Moby Dick, and how often do they occur?\n",
    "\n",
    "Note that the HTML file you are asked to request is a cashed version of this file from Project Gutenberg.\n",
    "\n",
    "Your project will follow these steps:\n",
    "\n",
    "The first step will be to request the Moby Dick HTML file using requests and encoding it to utf-8. Here is the URL to scrape from: https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\n",
    "Next, you'll extract the HTML and create a BeautifulSoup object using an HTML parser to get the text.\n",
    "Following that, you'll initialize a regex tokenizer object tokenizer using nltk.tokenize.RegexpTokenizer to keep only alphanumeric text, assigning the results to tokens.\n",
    "You'll transform the tokens into lowercase, removing English stop words, and saving the results to words_no_stop.\n",
    "Finally, you'll initialize a Counter object and find the ten most common words, saving the result to top_ten and printing to see what they are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0",
   "metadata": {
    "collapsed": false,
    "executionTime": 35,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... "
   },
   "outputs": [],
   "source": [
    "# Import and download packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Start coding here... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05225082-9bc8-4475-ada4-f1b31cf44386",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm'\n",
    "\n",
    "reponse = requests.get(url)\n",
    "\n",
    "# Make the GET request\n",
    "if response.status_code == 200:\n",
    "    response.encoding = 'utf-8'\n",
    "    # Get the HTML content\n",
    "    html_content = response.text\n",
    "    print(html_content[0:500])\n",
    "else:\n",
    "    print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9ae96-b2de-4d55-a436-4443347041d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "body_dick = soup.body.get_text()\n",
    "\n",
    "print(len(body_dick))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d10f6b-5b32-4bc3-8ea9-e979a2b16ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cleaned_body_dick = re.sub(r'[\\n\\r]+', ' ', body_dick)\n",
    "cleaned_body_dick = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned_body_dick)\n",
    "\n",
    "print(len(cleaned_body_dick))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7bb8e-904c-4c75-93c9-9d8d2a7e5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokens = tokenizer.tokenize(cleaned_body_dick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa92b9c-f3e1-4923-b48b-edcc3fb6a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd96b90-058b-4412-aef1-0075c03da853",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d075582-c06a-48cc-a36b-5c45226fac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed53cb3-f150-4325-8c8d-04e898ba1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_no_stop = [w for w in lower_tokens if not w in english_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7c2c9-4db2-4c2b-a478-ab2977e9c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fd6f5-e5c9-4614-8fd8-75822803900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(words_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16805ef3-848a-445f-9e30-70fdf06ed542",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten = dict(sorted(counter.items(), key=lambda item: item[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8e3c8-a40a-4cca-9fd4-40fdebf61a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b9104-0319-46b2-adfa-e69f494ade92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a233b2e-4ea3-4d51-9c09-c5914b359f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
