{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a608043-dd37-4b5f-8fd2-d90251f1eca2",
   "metadata": {},
   "source": [
    "# Data Scientist Example Practical Exam Solution - Tasty Bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb88065-a015-4221-a20a-cee13a357c10",
   "metadata": {},
   "source": [
    "## Data Validation\n",
    "This data set has 947 observations, 8 features (being one of them the target variable). I have validated all variables and I have identified few violations. Data dictionary with the validation issues identified:\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8da7aa-e536-4103-9daa-ccca3dbe208b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T13:21:16.150281Z",
     "iopub.status.busy": "2024-11-06T13:21:16.148887Z",
     "iopub.status.idle": "2024-11-06T13:21:16.169281Z",
     "shell.execute_reply": "2024-11-06T13:21:16.168206Z",
     "shell.execute_reply.started": "2024-11-06T13:21:16.150196Z"
    }
   },
   "source": [
    "| Column       | Type      | Description                                                     | Validation Issues                                              |\n",
    "|--------------|-----------|-----------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| recipe       | Numeric   | Unique identifier of recipe                                     | As described. No cleaning needed.                              |\n",
    "| calories     | Numeric   | Number of calories                                              | Type as described. 52 missing values.                          |\n",
    "| carbohydrate | Numeric   | Amount of carbohydrates in grams                                | Type as described. 52 missing values.                          |\n",
    "| sugar        | Numeric   | Amount of sugar in grams                                        | Type as described. 52 missing values.                          |\n",
    "| protein      | Numeric   | Amount of protein in grams                                      | Type as described. 52 missing values.                          |\n",
    "| category     | Character | Type of recipe. One of ten possible categories listed           | One extra category ('Chicken Breast') in the dataset.          |\n",
    "| servings     | Numeric   | Number of servings for the recipe                               | Three values contain non-numeric characters.                   |\n",
    "| high_traffic | Character | Indicates if the traffic to the site was high for this recipe   | Values marked with \"High\" if high traffic.                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97998039-e550-43e6-a245-5f13715c0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f92f4-3e81-4922-b60f-af7a8ed566e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('recipe_site_traffic_2212.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e652abe-67d2-421c-9986-4e006f0c8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc98e26-c1f0-49a4-ba58-ae08b9202d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the presence of null values in the dataset\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72003ef-1157-4c2c-a099-18b259a9b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the null values in the calories, carbo., sugar and protein.\n",
    "df[df['calories'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd148d8-e393-4035-8333-7a20424cddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate recipe ids?\n",
    "df['recipe'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3888e6b-8587-4e21-ab5a-61289f8bb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the list of categories\n",
    "df['category'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f951d-8b06-4877-834e-96de306d4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification of violating category\n",
    "expected_cats = set(['Lunch/Snacks', 'Beverages', 'Potato', 'Vegetable', 'Meat', 'Chicken', 'Pork', 'Dessert', 'Breakfast', 'One Dish Meal'])\n",
    "existing_cats = set(df['category'].unique().tolist()) \n",
    "extra_cat = existing_cats.difference(expected_cats)\n",
    "print(extra_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cba31-3267-447d-8bd7-dd74106208b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the violating category is a variety of one existing category, lets convert it\n",
    "df.loc[df['category']=='Chicken Breast', 'category'] = 'Chicken'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434093c-7e73-435b-9f83-8ea6a89e0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# are categories ok now?\n",
    "existing_cats = set(df['category'].unique().tolist()) \n",
    "extra_cat = existing_cats.difference(expected_cats)\n",
    "print(extra_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa898c3f-b443-43a8-a8c9-a75be7a30dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview on servings\n",
    "df['servings'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc30fdf-c36e-482b-8c3e-dcab509eb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is servings numeric?\n",
    "df['servings'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f474a96-e1ad-4bbd-a4ff-ad33a1d9515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the violating values to valid ones and adapt the type\n",
    "df.loc[df['servings']=='4 as a snack', 'servings'] = '4'\n",
    "df.loc[df['servings']=='6 as a snack', 'servings'] = '6'\n",
    "df['servings'] = df['servings'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116aea2-c9e1-47c3-aebf-e656113c3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High traffic values \n",
    "df['high_traffic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c6d46-a889-4578-a69f-f913a8239e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get rid of the null values in the target variable by converting high_traffic into a boolean\n",
    "df['high_traffic'] = df['high_traffic']=='High' \n",
    "df['high_traffic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3bd30b-5c58-478a-9a90-c3bc261628f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only data validation problem left to be tackled are the missing values in the nutritional info columns. 52 rows are 5% of the observations. That is too simply remove them.\n",
    "# Lets see how the different nutriotional features vary regarding category\n",
    "\n",
    "nutritional_cols = ['protein', 'sugar', 'carbohydrate', 'calories']\n",
    "\n",
    "for feature in nutritional_cols:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.boxplot(data=df, hue='category', y=feature, linewidth=0.5)\n",
    "    plt.title('{} distribution by category'.format(feature))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e230c-45b1-4c7f-a9ab-853e68113abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the same for servings\n",
    "\n",
    "for feature in nutritional_cols:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.boxplot(data=df, hue='servings', y=feature, linewidth=0.5)\n",
    "    plt.title('{} distribution by servings'.format(feature))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f01463-639a-4243-8ff5-e9ab86c7a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the distributions of the nutritional features seems impacted by the category and not by the servings, \n",
    "# lets use the category to impute means to the missing nutritional observations\n",
    "\n",
    "for column in nutritional_cols:\n",
    "    df[column] = df.groupby('category')[column].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1650eb-d7e8-4a95-a265-51cf97bc94be",
   "metadata": {},
   "source": [
    "### Post Data Validation Corrections\n",
    "\n",
    "After having fixed the violations identified in the dataset compared to the provided data dictionary, the dataset remains as follows: \n",
    "\n",
    "| Column       | Type      | Actions undertaken                                                                                           |\n",
    "|--------------|-----------|-------------------------------------------------------------------------------------------------------------|\n",
    "| recipe       | Numeric   | No cleaning needed.                                              |\n",
    "| calories     | Numeric   | 52 missing values filled with the mean of the **category**.                              |\n",
    "| carbohydrate | Numeric   | 52 missing values filled with the mean of the **category**.                |\n",
    "| sugar        | Numeric   | 52 missing values filled with the mean of the **category**.                        |\n",
    "| protein      | Numeric   | 52 missing values filled with the mean of the **category**.                      |\n",
    "| category     | Character | One extra category value ('Chicken Breast') merged with 'Chicken'.                           |\n",
    "| servings     | Numeric   | Non-numeric characters converted with no information loss.                |\n",
    "| high_traffic | Boolean   | Converted to boolean where True indicates 'High' and False indicates not 'High'.                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6043e4-fd2f-4b46-a4fd-72f275bc82ed",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "\n",
    "After having investigated all the features and their relationship with the target variable I decided to apply the following changes to ease the modeling part:\n",
    "- log transformation to the nutritional features\n",
    "- servings as category\n",
    "- drop recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a1688-1ff8-4000-aded-b96818f1569c",
   "metadata": {},
   "source": [
    "### Target Variable - high_traffic\n",
    "\n",
    "*high_traffic* is the feature we are trying to predict. It is unbalanced (60~40)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b4a17e-417c-43d8-acbe-2e50702cf8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(data=df, x='high_traffic', hue='high_traffic')\n",
    "plt.title('Distribution of the target variable (high_traffic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687572b2-680d-47fe-8910-6f416fe0c90b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:49:49.943008Z",
     "iopub.status.busy": "2024-11-06T08:49:49.941647Z",
     "iopub.status.idle": "2024-11-06T08:49:49.965665Z",
     "shell.execute_reply": "2024-11-06T08:49:49.963884Z",
     "shell.execute_reply.started": "2024-11-06T08:49:49.942919Z"
    }
   },
   "source": [
    "### Numeric Variables - Calories, Sugar, Protein, Carbohydrate and Servings\n",
    "\n",
    "The distributions of the nutritional features (calories, sugar, protein and carbo.) are right skewed. To facilitate modeling its better to have data normally distributed, since numerous models have it as an assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba318f6-98c3-4d2e-90e4-df17108900b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see first how the nutritional columns are distributed\n",
    "fig, axes = plt.subplots(1,4,figsize=(15,5))\n",
    "sns.histplot(data=df, x='calories', ax=axes[0]).set(title='Distribution of Calories')\n",
    "sns.histplot(data=df, x='sugar', ax=axes[1]).set(title='Distribution of Sugar')\n",
    "sns.histplot(data=df, x='protein', ax=axes[2]).set(title='Distribution of Protein')\n",
    "sns.histplot(data=df, x='carbohydrate', ax=axes[3]).set(title='Distribution of Carbo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6daf32a-8895-4099-b573-47295ebd61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the distributions are right skewed, lets appy a log transformation to try to normalize them\n",
    "for col in nutritional_cols: \n",
    "    df[col + '_log'] = np.log1p(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee5656-b863-4afb-a98e-9043aebf266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,4,figsize=(15,5))\n",
    "sns.histplot(data=df, x='calories_log', ax=axes[0]).set(title='Distribution of log(Calories)')\n",
    "sns.histplot(data=df, x='sugar_log', ax=axes[1]).set(title='Distribution of log(Sugar)')\n",
    "sns.histplot(data=df, x='protein_log', ax=axes[2]).set(title='Distribution of log(Protein)')\n",
    "sns.histplot(data=df, x='carbohydrate_log', ax=axes[3]).set(title='Distribution of log(Carbo.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d8341-7bb1-485b-af52-e5964c6b9ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once transformed, lets see how the different nutritional features relate to the target variable\n",
    "fig, axes = plt.subplots(2,2,figsize=(15,5))\n",
    "sns.boxplot(data=df, y='calories_log', hue='high_traffic', width=0.3, ax=axes[0][0]).set(title='Relation log(Calories) and High_Traffic')\n",
    "sns.boxplot(data=df, y='sugar_log', hue='high_traffic', width=0.3, ax=axes[0][1]).set(title='Relation log(Sugar) and High_Traffic')\n",
    "sns.boxplot(data=df, y='protein_log', hue='high_traffic', width=0.3, ax=axes[1][0]).set(title='Relation log(Protein) and High_Traffic')\n",
    "sns.boxplot(data=df, y='carbohydrate_log', hue='high_traffic', width=0.3, ax=axes[1][1]).set(title='Relation log(Carbo.) and High_Traffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be4a116-3616-4ec5-ba51-66850e59acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how the different servings observations are distributed\n",
    "sns.countplot(data=df, x='servings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629061bb-d0ca-4d36-ba3e-72a312bec6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see now how servings is linked to the target variable\n",
    "sns.countplot(data=df, x='servings', hue='high_traffic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4cab2-d083-4d75-9735-fcda2d23fd11",
   "metadata": {},
   "source": [
    "### Categorical Variables - Category\n",
    "\n",
    "After analysing the category feature and its relationship with the target variable it seems to be the the feature that explains the best the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601eb187-8f46-47a7-a676-fdb22801b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets convert category into a categorical feature.\n",
    "df['category'] = df['category'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64acc316-8c9c-438b-bc60-f9ca550e57f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And see how many samples of each category we have\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(data=df, x='category', hue='category', palette=\"dark\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution of categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d3fd06-a315-4f80-9e85-9638824dc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And lets see now how the different recipes are splitted into high traffic or not by category\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(data=df, x='category', hue='high_traffic')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Relationship categories and target variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed54c4-1b67-4f78-8d9c-0d1cea0d3ae7",
   "metadata": {},
   "source": [
    "So far, the *category* feature seems to be the one being able to best explain the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a374058-843d-41db-a236-003c707a861b",
   "metadata": {},
   "source": [
    "## Model Fitting & Evaluation\n",
    "\n",
    "Predicting a boolean variable is a clasification problem in machine learning. Due to the small number of features and observations I am picking **Logistic Regression** as the model of choice and I will add a **Gradient Boosting Classifier** to compare. Both are interpretable and should be flexible enough to predict the target variable in this case. \n",
    "\n",
    "To ensure the best possible hyperparameters, I will perform a grid search during the training process. The model’s performance will be evaluated using cross-validation to avoid overfitting and provide a more reliable estimate of its generalization ability.\n",
    "\n",
    "For the final evaluation, the two metrics I am choosing are **accuracy** and **precision**. Accuracy provides an overall sense of how well the model is predicting both classes correctly. However, precision is particularly important for this scenario because it focuses on the positive class (high traffic recipes) and indicates the percentage of correctly identified high traffic recipes out of all recipes predicted as high traffic.\n",
    "\n",
    "In our specific case, we aren’t concerned if a true high traffic recipe does not make it to the homepage. What we want to avoid is incorrectly displaying a low traffic. Therefore, precision will help minimize these false positives and align with our objective.\n",
    "\n",
    "A baseline we could use to compare our models is the fact that randomly picking a recipe from the provided dataset to be displayed on the home page will have an accuracy (and a precission) of 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dda92c-7397-4cf3-ae0f-4255ae4a79a1",
   "metadata": {},
   "source": [
    "## Prepare Data for Modelling\n",
    "\n",
    "I am going to use all the variables (but recipe) as features and the high_traffic column as target variable.\n",
    "\n",
    "On top of the imputation of the null values, the log transformation of the nutritional features, I have performed the following modifications on the input features:\n",
    "- Drop unused features\n",
    "- Normalize the numeric features\n",
    "- Convert the categorical variables into numeric features\n",
    "- Split the data into a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576e069-5d15-46b1-9051-401af8328b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['recipe', 'calories', 'sugar', 'protein', 'carbohydrate'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742fb7a6-70df-4c87-8b3d-bdbdb8370c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_cols = ['calories_log', 'protein_log', 'sugar_log', 'carbohydrate_log']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c92e1-d9f3-4f22-88ff-f6c4300432c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['servings'] = df['servings'].astype('category')\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=['category', 'servings'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096438d-4e27-4aa0-af01-2bdc9cf740ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop('high_traffic', axis=1)\n",
    "y = df_encoded['high_traffic']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06adce4c-ca82-465c-b3b0-5b6c8351d50c",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb3a9b-a004-4e3d-92b0-825ff3c36cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models and parameters\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1500, class_weight={0: 2, 1: 1}),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Parameter grids\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [20, 40, 70, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [2, 4, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Capture the trained models\n",
    "trained_models={}\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining Model: {model_name}\")\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], \n",
    "                               scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = best_model.predict(X_train)\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_test)}\")\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "    # Capture trained model \n",
    "    trained_models[model_name] = best_model\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25eb62-7896-49f7-b988-499b5abd72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feat_imp(feature_importance, model_name):\n",
    "    feature_importance['Importance'] = feature_importance['Importance'].abs() \n",
    "    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(\n",
    "        data=feature_importance,\n",
    "        x='Importance',\n",
    "        hue='Feature',\n",
    "        palette='Spectral'\n",
    "    )\n",
    "    plt.title('Feature Importances ({} Coefficients)'.format(model_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f5885-2037-4796-bf79-05c2e4c5d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importances of the logistic regression model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': trained_models['Logistic Regression'].coef_[0]\n",
    "})\n",
    "\n",
    "display_feat_imp(feature_importance, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importances of the gradient boosting model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': trained_models['Gradient Boosting'].feature_importances_\n",
    "})\n",
    "\n",
    "display_feat_imp(feature_importance, 'Gradient Boosting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa04e6-b96b-4fde-8dc7-eff06667a7f2",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "In terms of **test accuracy** the Logistic model beats Gradient Boosting probably because the later seems to be overfitting the training data (higher training accuracy than test accuracy). The **precission**, or the ability to _minimize false positives_ is higher in the Logistic Regression model too. So for this scenario, we have a clear winner. Since the best model has been found using Grid Search I dont estimate necessary to fine tune the best parameters already found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5fcf2b-952f-4ce8-bcdd-ba61c0a3a0d4",
   "metadata": {},
   "source": [
    "## Evaluation based on Business Criteria\n",
    "\n",
    "Tasty Bytes wants to be able to predict recipes that will lead to high traffic, and be able to predict high traffic recipes 80% of the time. \n",
    "\n",
    "With the Logistic Regression model we have trained we can even go beyond the initial business requirements and provide a tool that, when predicting a high traffic recipe, will be right 88% of the time. \n",
    "\n",
    "We can actually monitor this with a KPI that could be defined as *the percentage of high traffic recipes displayed on the home page**, assuming these recipes have been picked by the tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e9326-5f3e-4286-a784-b277339d35ee",
   "metadata": {},
   "source": [
    "## Recommendation \n",
    "\n",
    "In order to increase the consistency of the success of the recipes displayed on the home page, remove the dependency towards the expertise of the person manually picking the recipes and to increase the level of automation of Tasty Bytes we recommend to deploy the Logistic Regression model and start using it as soon as possible. \n",
    "\n",
    "In a first phase, I would recommend to use it to assist the manual selection currently happening, to eventually identify and fix errors, fine tune the model in necessary, etc. \n",
    "\n",
    "I would like to assess how easy it could be to get the rest of the data currently in the platform like ingredients, cost per serving, time to make... that i saw on the website. These features could have a positive impact on the performance of the model, and if they are readily available it would be worthy to check their inclusion. \n",
    "\n",
    "In any case, once we get the OK from the person in charge, In a second phase I would deploy the model in production and automate the selection of the recipe to be included in the home page using the predictions our model outputs.\n",
    "\n",
    "Along the way, I would keep collecting data to monitor the performance of the model to identify performance drops, eventuals data or concept drifts and retrain it regularly as new data is available.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1078c0bb-5e70-465e-b862-a3d01f07eeac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
