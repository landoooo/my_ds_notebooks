{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb7c51-22bb-4c35-bc64-f7e758effea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d4c74-73d7-4e6b-a16e-612a3184a688",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Data preprocessing takes place after exploratory data analysis and cleaning\n",
    "\n",
    "We preprocess the data to: \n",
    "- transform the dataset so its suitable for modeling\n",
    "  AND\n",
    "- to improve model performance\n",
    "\n",
    "## 1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff760c-039a-4f25-a834-29011d9e4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/volunteer_opportunities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee03e9-7289-4749-984f-de0f7e39d8bf",
   "metadata": {},
   "source": [
    "## 2. Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716f5ec-c64d-45cd-a2ee-76e68950fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba63af-3d3b-423e-bfdb-2ddddd896253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73168b8-7566-4dd7-9281-dddfd583e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd341c8e-0c57-49b4-9c03-e5c679dc59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91190ac2-e856-496a-b634-8422cf3f2c30",
   "metadata": {},
   "source": [
    "## 3. Remove Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a5626-4b2f-4429-b40f-4ef36c8a4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8214e-ba78-4eab-8d02-e6613b4036b9",
   "metadata": {},
   "source": [
    "- *df.dropna()* -> if only a few rows contain missing data\n",
    "- *df.drop([1,2,3])* -> drops specific rows\n",
    "- *df.drop('column_name', axis=1)* -> drops columns\n",
    "- *df.dropna(subset=['column_name'])* -> drops rows where column_name is empty\n",
    "- *df.dropna(thresh=2)* -> drop columns with 2 or more missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b096385-a5f2-45cf-978c-42231d2a1453",
   "metadata": {},
   "source": [
    "## 4. Typing\n",
    "\n",
    "Pandas infer data types, sometimes incorrectly.\n",
    "\n",
    "The *.info()* method shows the datatype of each column as well as *.dtypes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21a1dc-d43b-4a30-86b2-8661bb771eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018f8ac-eccf-4246-9487-c5830142f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9afff-40b3-4831-bb4c-f71e3c4aa547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy = pd.DataFrame({'A': ['1.0', '2.0']})\n",
    "df_toy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236b80e-36fd-4c9d-bd5e-d3d025a33bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy['A'] = df_toy['A'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed4e0a-2f81-4de8-990a-b45b15b3b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b1241-b169-476d-800a-09cc45a92725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f00c8-6f36-4fa9-8bd6-92e1175544e8",
   "metadata": {},
   "source": [
    "## 5. Training and test split\n",
    "\n",
    "Splitting the dataset into training and test helps: \n",
    "- reducing overfitting\n",
    "- evaluate performance on a holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105344b5-6485-44e7-ad9a-58c247d4304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df.category_desc, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3003b-9feb-4b8f-b744-655e50e71878",
   "metadata": {},
   "source": [
    "**Stratified sampling** helps keeping all the classes represented in the target test dataset when it is very imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab813734-d378-4a7e-86af-2a7282ff8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, df.category_desc, test_size=0.2, random_state=42, stratify=df.category_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39dbcf-ccb1-4777-a1be-d619a93dc7db",
   "metadata": {},
   "source": [
    "## 6. Normalization\n",
    "\n",
    "Unifying the scales of features, making their values in [0,1]\n",
    "\n",
    "sklearn.proprocessing.Normalizer\n",
    "\n",
    "## 6. Standardization\n",
    "\n",
    "**Standardization** is the process to transform **continuous** data to appear **normally distributed**\n",
    "\n",
    "Many of the sklearn models assume normally distributed data. Using non-normal data could bias the models.\n",
    "\n",
    "Standardization is required: \n",
    "- When we are using a model in linera space (like KNN, linear regression or KMeans)\n",
    "- When the dataset features have high variance\n",
    "- Features are on different scales (for instance number of bedrooms vs price)\n",
    "\n",
    "### 6.1. Log Normalization\n",
    "\n",
    "Useful for features with high variance\n",
    "\n",
    "Applies logarithm transformation\n",
    "\n",
    "Natural log using the constant $e$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2120a02-f4ff-442e-955c-4fd7b94bdf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c\n",
    "df['logs'] = np.log(df['values'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e9394-08f8-443d-8368-9789b2a12925",
   "metadata": {},
   "source": [
    "Captures relative changes, the magnitude of change, and keeps everything positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a2250-ff81-4b8b-a038-6c92b1b50a4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T12:04:59.939936Z",
     "iopub.status.busy": "2024-08-27T12:04:59.938822Z",
     "iopub.status.idle": "2024-08-27T12:04:59.953136Z",
     "shell.execute_reply": "2024-08-27T12:04:59.951393Z",
     "shell.execute_reply.started": "2024-08-27T12:04:59.939891Z"
    }
   },
   "source": [
    "### 6.2. Scaling \n",
    "\n",
    "Features on different scales\n",
    "\n",
    "Model with linear characteristics\n",
    "\n",
    "Center features around 0 and transform variance to 1\n",
    "\n",
    "Transforms to approximately normal distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfda24c-85f2-4a9e-bc52-47a14faff13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'col1': [0.1, 0.2, 0.3],\n",
    "    'col2': [10, 5.2, 8.3],\n",
    "    'col3': [120, 100.2, 89.3]\n",
    "})\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f1704d-0ed6-4ec5-8770-818ac6f4a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bafb2f6-24ee-41d5-9de4-5125a71ae4dc",
   "metadata": {},
   "source": [
    "### 6.3. Standardized data and modeling\n",
    "\n",
    "Its important to split the data before preprocessing the data, otherwise there would be a **data leakage** and the test data could have been showed somehow to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e857d4a-1f51-458c-afe3-ae83823b114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "knn.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e53b593-2e0e-4145-828e-c0b91faab69f",
   "metadata": {},
   "source": [
    "### 6.4 Log transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290df49e-8f04-4f5d-a776-3a03f0bcf1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Sample data\n",
    "data = {'feature': [1, 12, 3,5, 45, 10, 100, 1000, 10000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a log transformation using sklearn's FunctionTransformer\n",
    "log_transformer = FunctionTransformer(np.log1p)  # log1p to handle log(0) gracefully\n",
    "\n",
    "# Apply the log transformation\n",
    "transformed_data = log_transformer.fit_transform(df)\n",
    "\n",
    "# Convert back to DataFrame for readability\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=df.columns)\n",
    "print(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec999eb-bda2-450a-91b6-4383876592a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e4e8a-5d6a-4afa-9eea-e4a91776af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17332651-fc93-4550-9d5b-2ab2b3e0248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Sample data\n",
    "data = {'feature': [1, 12, 3,5, 45, 10, 100, 1000, 10000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a log transformation using sklearn's FunctionTransformer\n",
    "log_transformer = PowerTransformer()\n",
    "\n",
    "# Apply the log transformation\n",
    "transformed_data = log_transformer.fit_transform(df)\n",
    "\n",
    "# Convert back to DataFrame for readability\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=df.columns)\n",
    "transformed_df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec4a11-dda8-4629-9cda-b1b69f9c316c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f5dd8f8-0b2e-40f5-ad14-ba9214ed930a",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "**Feature engineering** is the creation of new features from existing ones\n",
    "\n",
    "It adds information to the dataset that can improve the performance of the model or add insight into relationships between features\n",
    "\n",
    "Before doing feature engineering we must understand our data first.\n",
    "\n",
    "Feature engineering is highly dependent on the dataset we have at hand\n",
    "\n",
    "Typical feature engineering scenarios are extracting features from free text, or parsing strings containing dates.\n",
    "\n",
    "## 8. Encoding categorical variables\n",
    "\n",
    "Sklearn models requires numerical input only. If there is any categorical data, it has to be encoded.\n",
    "\n",
    "### 8.1 Encoding variables with 2 different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b1f24b-0743-4f99-ba43-a2672956692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'user': [1,2,3, 4],\n",
    "    'subscribed': ['y','y','n', 'y'],\n",
    "    'fav_color': ['yellow', 'orange', 'orange', 'green']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ecfce-5cba-4fec-a09e-b7ab2db643ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.subscribed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfaa7f-2d37-4433-904f-4935503adbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas way\n",
    "df.subscribed.apply(lambda x: 1 if x=='y' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f9950-6f33-4d74-86a1-8665947a909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciKit Learn way \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit_transform(df.subscribed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed210c04-4aba-4987-9136-26b2ff5a962e",
   "metadata": {},
   "source": [
    "### 8.2 One Hot Encoding\n",
    "\n",
    "Applies when the variable has more than 2 different values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506e115-14b5-4d61-a0be-29120ef81c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df.fav_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ab20a-4bb8-4e46-bb05-583b2807806c",
   "metadata": {},
   "source": [
    "One hot encoding a categorical variable with n different values ends up in n new columns. This presents a problem since one of the columns is redundant with all the rest (it could be calculated from the values of the others) what can cause issues in our models.\n",
    "\n",
    "> The pandas *.get_dummies()* method returns **one hot encoded columns** by default\n",
    "\n",
    "### 8.3 Dummies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b048b0d-ae4d-41c4-9df3-78c9f093b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df.fav_color, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61c88c-8f19-4842-8d35-dce4cfa5c4bf",
   "metadata": {},
   "source": [
    "### 8.4 Containing the total amount of features\n",
    "\n",
    "To avoid ending up with dozens of features, when encoding categorical variables is always recommendable to perform a *.value_counts()* on the categorical variable and think about grouping or dropping categories with a small amount of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960a783-b27c-4315-91da-2d4ef9c9fc0d",
   "metadata": {},
   "source": [
    "## 9. Engineering Numerical Features\n",
    "\n",
    "Examples of reducing dimensionality: Means or medians of several variables, extracting month or week from dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57300bc-8958-4354-93d5-aef570e7f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer = pd.read_csv('../data/volunteer_opportunities.csv')\n",
    "volunteer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83616549-b3c4-439d-bce8-1522e0f93fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2d0b5-5dab-4688-af89-d55cadcccea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
    "volunteer[\"start_date_month\"] = volunteer['start_date_converted'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd3162-8dd3-42e7-8a1c-cd2e3a1da132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(volunteer[['start_date_converted', 'start_date_month']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dbf795-c9d5-437f-a9f6-6665d7e481fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T13:19:41.950489Z",
     "iopub.status.busy": "2024-08-27T13:19:41.949710Z",
     "iopub.status.idle": "2024-08-27T13:19:41.958589Z",
     "shell.execute_reply": "2024-08-27T13:19:41.956069Z",
     "shell.execute_reply.started": "2024-08-27T13:19:41.950446Z"
    }
   },
   "source": [
    "## 10. Engineering Text Features\n",
    "\n",
    "Text data with no format at all is called **free txt**\n",
    "\n",
    "### 10.1. Cleanup\n",
    "\n",
    "- **[^a-zA-Z]** all letter characters\n",
    "- **[^a-zA-Z]** all non letter characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f0047-959a-45c6-9bbb-398ad5125c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer['cleaned_summary'] = volunteer['summary'].str.replace('[^a-zA-Z]', '')\n",
    "\n",
    "volunteer['cleaned_summary'] = volunteer['summary'].str.replace(',', '')\n",
    "\n",
    "volunteer['cleaned_summary'] = volunteer['summary'].str.lower()\n",
    "\n",
    "volunteer['cleaned_summary_len'] = volunteer['summary'].str.len()\n",
    "volunteer['cleaned_summary_words'] = volunteer['summary'].str.split().str.len()\n",
    "\n",
    "volunteer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cac6ed-d8d6-4057-b43d-b87348503fe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T13:19:41.950489Z",
     "iopub.status.busy": "2024-08-27T13:19:41.949710Z",
     "iopub.status.idle": "2024-08-27T13:19:41.958589Z",
     "shell.execute_reply": "2024-08-27T13:19:41.956069Z",
     "shell.execute_reply.started": "2024-08-27T13:19:41.950446Z"
    }
   },
   "source": [
    "### 10.2. Extraction\n",
    "\n",
    "Regular expressions is code that identify patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfb966-ba74-45ab-8d12-5e316c4cd496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "my_string = 'temperature:75.6 F'\n",
    "temp = re.search('\\d+\\.\\d+', my_string)\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad178ba1-d139-4861-a2ea-875317692cc7",
   "metadata": {},
   "source": [
    "If we are working with text it could be helpful to model it in some way.\n",
    "\n",
    "**TF/IDF** (Term Frequency/Inverse Document Frequency) Vectorizes words based upon importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa7edb-5b63-4b6b-9f15-ef72929516fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "volunteer.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42753ade-025f-4241-8749-956967722238",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "text_tfidf = tfidf_vec.fit_transform(volunteer.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3730ea-1de4-46d1-9def-1838ff6a960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87bd148-45d7-4b4a-9171-077de3ce42f4",
   "metadata": {},
   "source": [
    "### 10.3. Word Counts\n",
    "\n",
    "A way to represent free text in tabular data is word counts: one feature per word and the value as the number of appearances of that word in the text.\n",
    "\n",
    "Using such a thing straight away would end up in a lot of meaningless features.\n",
    "\n",
    "CountVectorizer has 2 params: \n",
    "- min_df: minimum fraction of documents the word must occur in\n",
    "- max_df: maximum fraction of documents the word can occr in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9088c2-c141-41d8-9224-2e9c332fbf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0.1, max_df=0.9) \n",
    "\n",
    "cv.fit(volunteer['cleaned_summary'])\n",
    "\n",
    "cv_transformed = cv.transform(volunteer['cleaned_summary'])\n",
    "\n",
    "cv_transformed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd967e71-216a-46c9-affe-e70ad35bd7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeeaece-f1f6-4364-8b60-fe1dc18ca02e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T13:57:50.654156Z",
     "iopub.status.busy": "2024-09-13T13:57:50.653123Z",
     "iopub.status.idle": "2024-09-13T13:57:50.662216Z",
     "shell.execute_reply": "2024-09-13T13:57:50.660814Z",
     "shell.execute_reply.started": "2024-09-13T13:57:50.654089Z"
    }
   },
   "source": [
    "### 10.4 TF-IDF Representation\n",
    "\n",
    "Word counts are simply and not very powerful since common words tend to be overrepresented.\n",
    "\n",
    "Term frequency - Inverse document Frequency (TF-IDF)\n",
    "$$\n",
    "\\frac{\\frac{\\text{Count of word occurrences}}{\\text{Total words in document}}}{\\log \\left( \\frac{\\text{Number of docs word is in}}{\\text{Total number of docs}} \\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64cc93-d911-4f66-aaf4-004f6b4ed48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english') \n",
    "\n",
    "tv.fit(volunteer['cleaned_summary'])\n",
    "\n",
    "tv_transformed = tv.transform(volunteer['cleaned_summary'])\n",
    "\n",
    "tv_transformed.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70582f6-581a-4700-b82e-b6e6c9466cdb",
   "metadata": {},
   "source": [
    "### 10.5. N-grams\n",
    "\n",
    "Words in isolation can lose their meaning. To avoid this problem of the bag-of-words implementation, n-grams. \n",
    "\n",
    "In n-grams we consider n consecutive words. Real meaning is better kept with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d314cfe-065e-4d21-ad07-29899b02975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(ngram_range=(2,2)) # ngram_range contains the n minimum and maximum of the n-grams \n",
    "\n",
    "tv.fit(volunteer['cleaned_summary'])\n",
    "\n",
    "tv_transformed = tv.transform(volunteer['cleaned_summary'])\n",
    "\n",
    "tv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef769d56-185d-41c1-9adb-c64eaf399dc7",
   "metadata": {},
   "source": [
    "## 11. Feature Selection \n",
    "\n",
    "Feature selection is the step where some features are picked from the feature set to be used for modeling.\n",
    "\n",
    "It doesnt create new features.\n",
    "\n",
    "The goal is to improve models performance\n",
    "\n",
    "There are many ways to perform feature selection, some are automated.\n",
    "\n",
    "Reducing noise, removing features strongly statistically correlated, reducing overall variance...\n",
    "\n",
    "sklearn.feature_selection.SelectfromModel\n",
    "\n",
    "### 11.1. Removing redundant features \n",
    "\n",
    "- Remove noisy features (almost the same information)\n",
    "- Remove highly correlated features. The Pearson correlation coefficient is handy for this.\n",
    "- Remove duplicated features (often created during feature engineering)\n",
    "- Remove features with repeated information (for instance latlon and city)\n",
    "- Remove the original values used to compute an aggregated value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b74a85-a418-4cfe-be61-7b2216bc862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.DataFrame({\n",
    "    'user': [-1,2,3, 4],\n",
    "    'other': [2, 1, 3, -1],\n",
    "    'another': [25,-24,2,1]\n",
    "})\n",
    "\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a85dc-2e5e-4139-bf5c-12ee57b7fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece89dee-291b-4276-9cc4-b1cde4b70d2e",
   "metadata": {},
   "source": [
    "### 11.2. Selecting features using text vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc908ec-92a3-4972-9424-2b3ed8136be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiking = pd.read_json('../data/hiking.json')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfid_vec = TfidfVectorizer()\n",
    "\n",
    "tfid_text = tfid_vec.fit_transform(hiking.Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b04a75-1a72-4120-a549-f2fcecd2d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d90256-9e46-4a56-87f6-7cd4a847540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_text[3].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f448e29-5e20-4e13-9f52-ba84fc5cc1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_text[3].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ab109-2cd2-4274-8dde-84ad480867fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c3a684-fd05-42b3-ada5-47a4e4a1ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_row = dict(zip(text_tfidf[3].indices, text_tfidf[3].data))\n",
    "zipped_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d592b-aa6b-4505-b46f-1046d4684eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the rest of the arguments\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, tfid_text, 8, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b03c54-0373-4a2e-a098-21030afaf4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# Filter the columns in text_tfidf to only those in filtered_words\n",
    "filtered_text = tfid_text[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6db2a3-9b3d-4595-8f69-346d03d6de18",
   "metadata": {},
   "source": [
    "## 12. Dimensionality Reduction and PCA\n",
    "\n",
    "Its an unsupervised learning method\n",
    "\n",
    "Combines/decomposes a feature space. It shrinks the number of features on the feature space.\n",
    "\n",
    "Dimensionality reduction is a feature extraction method since the data is transformed into new and different features.\n",
    "\n",
    "PCA stands for principal component analysis and uses a linear transformation to project features into a linear space where they can be completely uncorrelated.\n",
    "\n",
    "While the feature space is reduce, the variance is captured in a meaningful way by combining features into components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da416c-e201-43aa-be24-64285c94392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df_pca = pd.DataFrame({\n",
    "    'user': [-1,2,3, 4],\n",
    "    'other': [2, 1, 3, -1],\n",
    "    'trololo': [20, 11, -53, 21],\n",
    "    'another': [25,-24,2,1]\n",
    "})\n",
    "\n",
    "pca = PCA()\n",
    "df_pca = pca.fit_transform(df_pca)\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a10ed6-62bc-4992-b63d-846c78e25594",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d836e-0435-4c3b-8e10-a20c2230a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c62cd-5e84-4641-822c-9190e948c8af",
   "metadata": {},
   "source": [
    "PCA has though some caveats: \n",
    "- Its components are difficult to interpret\n",
    "- It fits better right at the end of the preprocessing journey. It would be hard to do anything with the pca components after its transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab7518-12e3-4a8d-80a3-07a11a90e75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
