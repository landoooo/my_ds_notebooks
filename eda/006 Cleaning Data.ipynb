{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af7c156-2594-4382-85aa-befe7acbf6a4",
   "metadata": {},
   "source": [
    "# Cleaning data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4ab90-2c37-47c3-8280-56f030f736b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf52f3b-9d47-4332-8b4c-3c85c09e8e40",
   "metadata": {},
   "source": [
    "**!! The current csv file doent contain whats expected to run the notebook**\n",
    "\n",
    "In the typical DS workflow we: \n",
    "1. Access the data\n",
    "2. Explore and process data\n",
    "3. Extract Insights\n",
    "4. Report Insights\n",
    "\n",
    "Dirty data can appear due to wrong encoding, duplicate entries, wrong processing, etc... If the raw data is garbage, we cannot expect but garbage to be output.\n",
    "\n",
    "## Ensuring the right data types\n",
    "\n",
    "First thing to check is that our variables have the right datatype. \n",
    "\n",
    "### Strings to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acecebc-f599-4776-bed1-c6552c618ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_sharing = pd.read_csv('../data/ride_sharing_new.csv')\n",
    "\n",
    "print(ride_sharing.info())\n",
    "\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
    "\n",
    "ride_sharing['user_type_cat'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb4379-feee-4bad-b6dd-6111a4635e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_sharing['duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9379193a-8545-442f-9dc7-068b0dece7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
    "\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
    "\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing.duration_time.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5236aff-8273-464d-8ab6-0a04b657fa8f",
   "metadata": {},
   "source": [
    "## Data range constraints\n",
    "\n",
    "When the range of a certain variable is known, it can happen that there are observations outside the range.\n",
    "\n",
    "What to do with out of range data:\n",
    "- Dropping data: go for this iff the amount of impacted data is low and after understanding the root cause of the issues.\n",
    "- Setting custom minimums or maximums\n",
    "- Setting custom value depending on business assumptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208d603-486d-4290-80e3-c6947858ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_sharing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f210758-0e41-4253-b9b0-325ef50eb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loaded dataframe dont contain tiresizes, but the idea is to cap the values going beyond the expected range to its max value.\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e8a84-d8d0-40b6-b44e-b417706bc783",
   "metadata": {},
   "source": [
    "## Handling duplicates\n",
    "\n",
    "Duplicates happen. Often they are not exact duplicates.\n",
    "They are originated because of: \n",
    "- Human errors\n",
    "- Bugs and design errors on business processes or pipelines\n",
    "- Joining/merging data sources\n",
    "\n",
    "How to find duplicates?\n",
    ".duplicated()\n",
    "\n",
    "How to remove duplicates?\n",
    ".drop_duplicates() \n",
    "\n",
    "We can use the duplicates to be removed to fulfill or improve the remaining records by using .groupby() and agg()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce272d47-593c-4899-97ad-27e73bb4dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = ride_sharing.duplicated(subset=['ride_id'], keep=False)\n",
    "\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307dde09-7c62-477f-ade3-07cb06a523b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
    "\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "assert duplicated_rides.shape[0] == 0Â£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58a2f4-ba3a-40af-88cf-b92e39123c27",
   "metadata": {},
   "source": [
    "## Categories and membership constraints\n",
    "\n",
    "Categorical data represent variables whose values belong to predefined set of categories. Variables like \"Marriage Status\", \"Household Income Category\" or \"Loan Status\" belong to this category.\n",
    "\n",
    "When feeding models, these variables are often converted into a numeric representation.\n",
    "\n",
    "Categorical variables cannot have values outside of the predefined ones.\n",
    "\n",
    "Inconsistencies in categorical variables may happen due to a variety of reasons: data entry errors, parsing errors...\n",
    "\n",
    "### Identifying inconsistent categories\n",
    "\n",
    "Before fixing the inconsistent categories we have to identify them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a43ca4-f2d5-4f0f-9143-a4305bdf008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['A', 'B', 'C']\n",
    "observations = ['A', 'B', 'D'] \n",
    "\n",
    "inconsistent_categories = set(observations).difference(categories)\n",
    "\n",
    "print(inconsistent_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d679823-58fe-4069-b8d4-c3191755d05a",
   "metadata": {},
   "source": [
    "We can now use the 'inconsistent_categories' with the isin pandas method to find observations with inconsistent values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b9f7a-5829-45eb-b7f8-f253a808fe2c",
   "metadata": {},
   "source": [
    "How do we treat these problems?\n",
    "\n",
    "### Dropping inconsistent categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d401740-7b85-4e58-97ad-b58dfa9e7f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.DataFrame({'cleanliness': {\n",
    "                                0: 'Clean',\n",
    "                                1: 'Average',\n",
    "                                2: 'Somewhat clean',\n",
    "                                3: 'Somewhat dirty',\n",
    "                                4: 'Dirty'\n",
    "                            },'safety': {\n",
    "                                0: 'Neutral',\n",
    "                                1: 'Very safe',\n",
    "                                2: 'Somewhat safe',\n",
    "                                3: 'Very unsafe',\n",
    "                                4: 'Somewhat unsafe'\n",
    "                            },'satisfaction': {\n",
    "                                0: 'Very satisfied',\n",
    "                                1: 'Neutral',\n",
    "                                2: 'Somewhat satisfied',\n",
    "                                3: 'Somewhat unsatisfied',\n",
    "                                4: 'Very unsatisfied'\n",
    "                            }\n",
    "                          }\n",
    ")\n",
    "\n",
    "airlines = pd.read_csv('../data/airlines_final.csv')\n",
    "\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1d07c-b95d-487d-87c3-89e24742fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "airlines[~cat_clean_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0587a-9a6f-496e-b801-210cd82f538c",
   "metadata": {},
   "source": [
    "### Other errors when dealing with categorical variables\n",
    "\n",
    "#### Value inconsistency\n",
    "Due to trailing spaces, wrong casing, spaces...\n",
    "\n",
    "We can str.upper() or str.lower() case to solve casing issues.\n",
    "\n",
    "str.strip() will solve the trailing spaces problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffc9d6-f4c7-4162-830d-955fd617f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85321d47-ded5-4475-9e6a-a9bf2f9f9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11393adb-3576-4df9-8ccc-f7b029caff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241ad0b-c292-466e-902b-d4b376380e79",
   "metadata": {},
   "source": [
    "#### Too many categories\n",
    "\n",
    "Often collapsable into few.\n",
    "\n",
    "For splitting into new categories there are two options: qcut (dumber) and cut (smarter). \n",
    "\n",
    "If we want to collapse already existing categories grouping them we can use the replace method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf9624-26fc-4285-9c8b-38d2c70dad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d9f83-e108-4802-bdf7-2adb51a104a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.wait_min.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca9da2-2540-48ae-8f54-158f6e499ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                                labels = label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc6366-4266-40eb-8830-684e24ab874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8643e-659f-4fc2-8622-0c2f9c8d668a",
   "metadata": {},
   "source": [
    "## Cleaning text data\n",
    "\n",
    "Text data is very frequent.\n",
    "\n",
    "Typical errors are typos, casing, excesive length, different formats...\n",
    "\n",
    "Regular expressions is a powerful tool for fixing these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd28e71-5a84-40cc-8af3-049a36183f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\",\"\")\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n",
    "\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a67ac-c4e6-428b-a5df-b3076ebb5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c800489-bb15-4d6b-89e6-587a11a5a607",
   "metadata": {},
   "source": [
    "## Uniformity\n",
    "\n",
    "Ensuring unit uniformity is paramount. Currency, temperatures, distances... are prone to have different units through datasets.\n",
    "\n",
    "Dates can have multiple formats too. Sometimes, specific dates can be ambiguous.\n",
    "\n",
    "Its very important to understand where the data is coming from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652c2b5-aa52-47a7-a0a5-fcc409067318",
   "metadata": {},
   "outputs": [],
   "source": [
    "banking = pd.read_csv('../data/banking_dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6544a-dd8e-48ff-bb02-d013902e54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ba557-bcef-4dbb-b59c-62bec802a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banking['account_opened'].head())\n",
    "\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "print(banking['acct_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c0ff7-90fd-4e74-a983-09d59468f3e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T14:40:54.579428Z",
     "iopub.status.busy": "2024-07-15T14:40:54.578490Z",
     "iopub.status.idle": "2024-07-15T14:40:54.606218Z",
     "shell.execute_reply": "2024-07-15T14:40:54.602414Z",
     "shell.execute_reply.started": "2024-07-15T14:40:54.579372Z"
    }
   },
   "source": [
    "### Cross Field Validation \n",
    "\n",
    "Cross field validation refers to the usage of multiple fields in a dataset to sanity check data integrity. This is often required when merging different datasets comming from different sources.\n",
    "\n",
    "In case of inconsistencies, many options can be taken into account, all of them requiring a deep understanding about where the data is comming from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f2e7e-a580-4779-9626-9a3adaf27390",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "inv_equ = banking[fund_columns].sum(axis=1) == banking['inv_amount']\n",
    "\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2296323-e25e-46e3-8a9d-96e460a7db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = dt.date.today()\n",
    "banking['birth_date'] = pd.to_datetime(banking['birth_date'])\n",
    "\n",
    "ages_manual = 2020 - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = banking['Age'] == ages_manual\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af70c90-f504-4023-b99b-b9a6ddb480bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "banking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be05a95-720a-4001-83ce-215584849df8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T14:55:30.848105Z",
     "iopub.status.busy": "2024-07-15T14:55:30.847215Z",
     "iopub.status.idle": "2024-07-15T14:55:30.855613Z",
     "shell.execute_reply": "2024-07-15T14:55:30.854424Z",
     "shell.execute_reply.started": "2024-07-15T14:55:30.848038Z"
    }
   },
   "source": [
    "### Completeness and missing data\n",
    "\n",
    "Missing data occurs when no data value is stored for a variable in an observation. Can be represented as NaN, NA, 0...\n",
    "\n",
    "There are multiple reasons behind missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4404a-8cfe-4fde-b027-72c10174cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get an idea about missingness\n",
    "banking.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52043166-5dc8-497f-a2c8-a7b3d17b8dd4",
   "metadata": {},
   "source": [
    "There is no null value in this dataframe\n",
    "\n",
    "The **missingno** package allows us to better understand our missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3759be04-b69c-4137-8272-5bfb636a7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "msno.matrix(banking)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a12aa-940b-4058-a766-ee4ec7899d03",
   "metadata": {},
   "source": [
    "Missing values can happen:\n",
    "- Completely at random: completely due to randomness\n",
    "- At random: there is a relationship between missing data and other observed values\n",
    "- Not at random: there is a systematic relationship between missing data and unobserved values\n",
    "\n",
    "Again, there are many ways to proceed with missing data: dropping it, imputing means, medians or modes, imputing based on machine learning models, impute them by hand if we have sufficient domain knowledge..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f960f37-6f27-43d0-b979-f2508ff0540b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:34:16.723904Z",
     "iopub.status.busy": "2024-07-15T15:34:16.723067Z",
     "iopub.status.idle": "2024-07-15T15:34:16.737543Z",
     "shell.execute_reply": "2024-07-15T15:34:16.736239Z",
     "shell.execute_reply.started": "2024-07-15T15:34:16.723851Z"
    }
   },
   "source": [
    "## Record Linkage\n",
    "\n",
    "**Minimum edit distance**: least possible amount of steps needed to transition from one string to another, operations being insertion, deletion, substitution or transposition. The smaller the minimum edit distance, the closer the 2 words are. There are several algorithms to calculate this distance.\n",
    "\n",
    "Several packages to calculate these distances: thefuzz, nltk, recordlinkage...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4cb48-c22d-4fa4-8012-a73faba252af",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = pd.read_csv('../data/restaurants_L2_dirty.csv')\n",
    "restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc094cf-3c2a-4359-83ee-38ffcc3206b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import process\n",
    "\n",
    "unique_types = restaurants.type.unique()\n",
    "\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
    "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
    "print(process.extract('italian', unique_types, limit = len(unique_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87355e2f-38fe-4468-95c0-5ee372c4838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restaurants['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee3ab4-4fab-416e-868b-6f9e0361bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = process.extract('italian', restaurants['type'], limit=len(restaurants))\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f12b2b-833d-4614-99fb-9eca553af2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = process.extract('italian', restaurants['type'], limit=len(restaurants.type))\n",
    "\n",
    "for match in matches:\n",
    "  if match[1] >= 80:\n",
    "    restaurants.loc[restaurants['type'] == match[0]] = 'italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28d350-4bb8-4db1-87cd-60d017de5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cuisine in categories:  \n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['type'], limit=len(restaurants.type))\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  for match in matches:\n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "      restaurants.loc[restaurants['type'] == match[0]] = cuisine\n",
    "      \n",
    "# Inspect the final result\n",
    "print(restaurants['type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169b3ba-df8f-4326-9b11-f552c078c897",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T16:39:49.272496Z",
     "iopub.status.busy": "2024-07-15T16:39:49.271709Z",
     "iopub.status.idle": "2024-07-15T16:39:49.284030Z",
     "shell.execute_reply": "2024-07-15T16:39:49.282438Z",
     "shell.execute_reply.started": "2024-07-15T16:39:49.272456Z"
    }
   },
   "source": [
    "**Record Linkage** is the act of linking data from different sources regarding the same entity. Generally, we clean two or more DataFrames, generate pairs of potentially matching records, score these pairs according to string similarity and other similarity metrics, and link them. All of these steps can be achieved with the recordlinkage package\n",
    "\n",
    "https://recordlinkage.readthedocs.io/en/latest/\n",
    "\n",
    "### Generating pairs\n",
    "First step for record linkage: composing candidate pairs for further comparison.\n",
    "Blocking is the technique to reduce the amount of pairs based on variables having the same value on both observations of the same pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba8bbe-5acb-4c47-8d13-a40fb7653838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage \n",
    "\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurants, restaurants_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951be84-858a-4e50-b3ff-50af4165a4c5",
   "metadata": {},
   "source": [
    "### Comparing fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cfedd-5923-4b79-b606-643dec475017",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, cuisine_types \n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('type', 'type', label = 'cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) \n",
    "\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5957ccaa-345f-4ca5-984c-6920e8bc1eb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T16:58:08.752213Z",
     "iopub.status.busy": "2024-07-15T16:58:08.750377Z",
     "iopub.status.idle": "2024-07-15T16:58:08.760365Z",
     "shell.execute_reply": "2024-07-15T16:58:08.758985Z",
     "shell.execute_reply.started": "2024-07-15T16:58:08.752143Z"
    }
   },
   "source": [
    "### Linking dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29609a-f8ad-43d6-99bb-8800ae9f269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.append(non_dup)\n",
    "print(full_restaurants)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
